{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import keras_tuner as kt\n",
    "from keras.callbacks import EarlyStopping\n",
    "import seaborn as sns\n",
    "from IPython.core.display_functions import display\n",
    "from IPython.display import Audio\n",
    "from livelossplot import PlotLossesKeras\n",
    "\n",
    "import keras\n",
    "from keras import layers\n",
    "from keras.models import Sequential\n",
    "\n",
    "from keras_preprocessing.image import img_to_array\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.layers import *\n",
    "\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "\n",
    "from music_plots import *\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "from sklearn.metrics import classification_report, ConfusionMatrixDisplay, confusion_matrix"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "<h1> Data management </h1>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "tracks_df = load(\"data/tracks.csv\")\n",
    "genres_df = load(\"data/genres.csv\")\n",
    "features_df = load(\"data/features.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "          #tracks  parent          title  top_level\ngenre_id                                           \n1            8693      38    Avant-Garde         38\n2            5271       0  International          2\n3            1752       0          Blues          3\n4            4126       0           Jazz          4\n5            4106       0      Classical          5",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>#tracks</th>\n      <th>parent</th>\n      <th>title</th>\n      <th>top_level</th>\n    </tr>\n    <tr>\n      <th>genre_id</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>8693</td>\n      <td>38</td>\n      <td>Avant-Garde</td>\n      <td>38</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5271</td>\n      <td>0</td>\n      <td>International</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1752</td>\n      <td>0</td>\n      <td>Blues</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4126</td>\n      <td>0</td>\n      <td>Jazz</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>4106</td>\n      <td>0</td>\n      <td>Classical</td>\n      <td>5</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tracks_df.head()\n",
    "genres_df.head()\n",
    "#features_df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "track_id\n2             Hip-Hop\n3             Hip-Hop\n5             Hip-Hop\n10                Pop\n134           Hip-Hop\n             ...     \n95823    Experimental\n95831    Experimental\n95866    Experimental\n95868    Experimental\n95871    Experimental\nName: genre_top, Length: 32823, dtype: category\nCategories (16, object): ['Blues', 'Classical', 'Country', 'Easy Listening', ..., 'Pop', 'Rock', 'Soul-RnB', 'Spoken']"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "track_genres = tracks_df.xs('track', level=0, axis=1)['genre_top'].loc[features_df.dropna().index]\n",
    "track_genres = track_genres.dropna()\n",
    "features_df = features_df.loc[track_genres.index]\n",
    "\n",
    "track_genres"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rock                   10040\n",
      "Experimental            7039\n",
      "Electronic              6201\n",
      "Hip-Hop                 2086\n",
      "Folk                    1773\n",
      "Pop                     1411\n",
      "Classical               1007\n",
      "International            879\n",
      "Instrumental             797\n",
      "Jazz                     517\n",
      "Old-Time / Historic      414\n",
      "Spoken                   307\n",
      "Soul-RnB                 130\n",
      "Country                  118\n",
      "Blues                     89\n",
      "Easy Listening            15\n",
      "Name: genre_top, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": "<AxesSubplot:>"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "value_counts = track_genres.value_counts()\n",
    "print(value_counts)\n",
    "value_counts.plot.bar()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Hip-Hop': 0, 'Pop': 1, 'Rock': 2, 'Experimental': 3, 'Folk': 4, 'Jazz': 5, 'Electronic': 6, 'Spoken': 7, 'International': 8, 'Soul-RnB': 9, 'Blues': 10, 'Country': 11, 'Classical': 12, 'Old-Time / Historic': 13, 'Instrumental': 14, 'Easy Listening': 15}\n"
     ]
    }
   ],
   "source": [
    "# genres dictionary\n",
    "genres = {}\n",
    "index = 0\n",
    "for i in track_genres.unique():\n",
    "    genres[i] = index\n",
    "    index += 1\n",
    "print(genres)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32823\n",
      "32823\n"
     ]
    },
    {
     "data": {
      "text/plain": "feature    chroma_cens                                                    \\\nstatistics    kurtosis                                                     \nnumber              01        02        03        04        05        06   \ntrack_id                                                                   \n2             7.180653  5.230309  0.249321  1.347620  1.482478  0.531371   \n3             1.888963  0.760539  0.345297  2.295201  1.654031  0.067592   \n5             0.527563 -0.077654 -0.279610  0.685883  1.937570  0.880839   \n10            3.702245 -0.291193  2.196742 -0.234449  1.367364  0.998411   \n134           0.918445  0.674147  0.577818  1.281117  0.933746  0.078177   \n...                ...       ...       ...       ...       ...       ...   \n95823        -0.555692 -0.338323  0.883295  0.424315  1.450894  0.070183   \n95831        -0.612445  0.702932  0.776201  0.186624  0.580003  0.287438   \n95866         3.528371  4.667782  5.247205  4.168351  3.276534  4.309053   \n95868        -0.033556  0.271865  0.280602  0.560396  1.511805  0.710866   \n95871         5.215266  4.964749  0.824476  1.733783  2.557416  3.057908   \n\nfeature                                             ...   tonnetz            \\\nstatistics                                          ...       std             \nnumber            07        08        09        10  ...        04        05   \ntrack_id                                            ...                       \n2           1.481593  2.691455  0.866868  1.341231  ...  0.054125  0.012226   \n3           1.366848  1.054094  0.108103  0.619185  ...  0.063831  0.014212   \n5          -0.923192 -0.927232  0.666617  1.038546  ...  0.040730  0.012691   \n10          1.770694  1.604566  0.521217  1.982386  ...  0.074358  0.017952   \n134         1.199204 -0.175223  0.925482  1.438509  ...  0.058766  0.016322   \n...              ...       ...       ...       ...  ...       ...       ...   \n95823       0.716997  0.757623  0.300295  0.063043  ...  0.064474  0.012939   \n95831      -0.037945 -1.151037 -1.074581  0.307664  ...  0.072024  0.011513   \n95866       2.768949  3.115098  3.815794  3.940277  ...  0.031871  0.012202   \n95868       1.451442  0.742910  0.739932  0.844851  ...  0.055034  0.012744   \n95871       1.768616  3.665696  1.518494  1.523711  ...  0.058086  0.016176   \n\nfeature                     zcr                                          \\\nstatistics             kurtosis       max      mean    median       min   \nnumber            06         01        01        01        01        01   \ntrack_id                                                                  \n2           0.012111   5.758890  0.459473  0.085629  0.071289  0.000000   \n3           0.017740   2.824694  0.466309  0.084578  0.063965  0.000000   \n5           0.014759   6.808415  0.375000  0.053114  0.041504  0.000000   \n10          0.013921  21.434212  0.452148  0.077515  0.071777  0.000000   \n134         0.015819   4.731087  0.419434  0.064370  0.050781  0.000000   \n...              ...        ...       ...       ...       ...       ...   \n95823       0.013397   6.294609  0.230469  0.039246  0.032715  0.004883   \n95831       0.013774  -0.943706  0.441406  0.117571  0.106934  0.005371   \n95866       0.012386   1.357928  0.400391  0.135901  0.143066  0.000000   \n95868       0.013205  -0.223611  0.402344  0.173291  0.182617  0.000000   \n95871       0.013788   0.166801  0.561035  0.211801  0.218262  0.000000   \n\nfeature                         \nstatistics      skew       std  \nnumber            01        01  \ntrack_id                        \n2           2.089872  0.061448  \n3           1.716724  0.069330  \n5           2.193303  0.044861  \n10          3.542325  0.040800  \n134         1.806106  0.054623  \n...              ...       ...  \n95823       1.909812  0.026319  \n95831       0.452720  0.085089  \n95866      -0.795436  0.055681  \n95868      -0.706943  0.048163  \n95871      -0.058994  0.087301  \n\n[32823 rows x 518 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead tr th {\n        text-align: left;\n    }\n\n    .dataframe thead tr:last-of-type th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr>\n      <th>feature</th>\n      <th colspan=\"10\" halign=\"left\">chroma_cens</th>\n      <th>...</th>\n      <th colspan=\"3\" halign=\"left\">tonnetz</th>\n      <th colspan=\"7\" halign=\"left\">zcr</th>\n    </tr>\n    <tr>\n      <th>statistics</th>\n      <th colspan=\"10\" halign=\"left\">kurtosis</th>\n      <th>...</th>\n      <th colspan=\"3\" halign=\"left\">std</th>\n      <th>kurtosis</th>\n      <th>max</th>\n      <th>mean</th>\n      <th>median</th>\n      <th>min</th>\n      <th>skew</th>\n      <th>std</th>\n    </tr>\n    <tr>\n      <th>number</th>\n      <th>01</th>\n      <th>02</th>\n      <th>03</th>\n      <th>04</th>\n      <th>05</th>\n      <th>06</th>\n      <th>07</th>\n      <th>08</th>\n      <th>09</th>\n      <th>10</th>\n      <th>...</th>\n      <th>04</th>\n      <th>05</th>\n      <th>06</th>\n      <th>01</th>\n      <th>01</th>\n      <th>01</th>\n      <th>01</th>\n      <th>01</th>\n      <th>01</th>\n      <th>01</th>\n    </tr>\n    <tr>\n      <th>track_id</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2</th>\n      <td>7.180653</td>\n      <td>5.230309</td>\n      <td>0.249321</td>\n      <td>1.347620</td>\n      <td>1.482478</td>\n      <td>0.531371</td>\n      <td>1.481593</td>\n      <td>2.691455</td>\n      <td>0.866868</td>\n      <td>1.341231</td>\n      <td>...</td>\n      <td>0.054125</td>\n      <td>0.012226</td>\n      <td>0.012111</td>\n      <td>5.758890</td>\n      <td>0.459473</td>\n      <td>0.085629</td>\n      <td>0.071289</td>\n      <td>0.000000</td>\n      <td>2.089872</td>\n      <td>0.061448</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1.888963</td>\n      <td>0.760539</td>\n      <td>0.345297</td>\n      <td>2.295201</td>\n      <td>1.654031</td>\n      <td>0.067592</td>\n      <td>1.366848</td>\n      <td>1.054094</td>\n      <td>0.108103</td>\n      <td>0.619185</td>\n      <td>...</td>\n      <td>0.063831</td>\n      <td>0.014212</td>\n      <td>0.017740</td>\n      <td>2.824694</td>\n      <td>0.466309</td>\n      <td>0.084578</td>\n      <td>0.063965</td>\n      <td>0.000000</td>\n      <td>1.716724</td>\n      <td>0.069330</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.527563</td>\n      <td>-0.077654</td>\n      <td>-0.279610</td>\n      <td>0.685883</td>\n      <td>1.937570</td>\n      <td>0.880839</td>\n      <td>-0.923192</td>\n      <td>-0.927232</td>\n      <td>0.666617</td>\n      <td>1.038546</td>\n      <td>...</td>\n      <td>0.040730</td>\n      <td>0.012691</td>\n      <td>0.014759</td>\n      <td>6.808415</td>\n      <td>0.375000</td>\n      <td>0.053114</td>\n      <td>0.041504</td>\n      <td>0.000000</td>\n      <td>2.193303</td>\n      <td>0.044861</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>3.702245</td>\n      <td>-0.291193</td>\n      <td>2.196742</td>\n      <td>-0.234449</td>\n      <td>1.367364</td>\n      <td>0.998411</td>\n      <td>1.770694</td>\n      <td>1.604566</td>\n      <td>0.521217</td>\n      <td>1.982386</td>\n      <td>...</td>\n      <td>0.074358</td>\n      <td>0.017952</td>\n      <td>0.013921</td>\n      <td>21.434212</td>\n      <td>0.452148</td>\n      <td>0.077515</td>\n      <td>0.071777</td>\n      <td>0.000000</td>\n      <td>3.542325</td>\n      <td>0.040800</td>\n    </tr>\n    <tr>\n      <th>134</th>\n      <td>0.918445</td>\n      <td>0.674147</td>\n      <td>0.577818</td>\n      <td>1.281117</td>\n      <td>0.933746</td>\n      <td>0.078177</td>\n      <td>1.199204</td>\n      <td>-0.175223</td>\n      <td>0.925482</td>\n      <td>1.438509</td>\n      <td>...</td>\n      <td>0.058766</td>\n      <td>0.016322</td>\n      <td>0.015819</td>\n      <td>4.731087</td>\n      <td>0.419434</td>\n      <td>0.064370</td>\n      <td>0.050781</td>\n      <td>0.000000</td>\n      <td>1.806106</td>\n      <td>0.054623</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>95823</th>\n      <td>-0.555692</td>\n      <td>-0.338323</td>\n      <td>0.883295</td>\n      <td>0.424315</td>\n      <td>1.450894</td>\n      <td>0.070183</td>\n      <td>0.716997</td>\n      <td>0.757623</td>\n      <td>0.300295</td>\n      <td>0.063043</td>\n      <td>...</td>\n      <td>0.064474</td>\n      <td>0.012939</td>\n      <td>0.013397</td>\n      <td>6.294609</td>\n      <td>0.230469</td>\n      <td>0.039246</td>\n      <td>0.032715</td>\n      <td>0.004883</td>\n      <td>1.909812</td>\n      <td>0.026319</td>\n    </tr>\n    <tr>\n      <th>95831</th>\n      <td>-0.612445</td>\n      <td>0.702932</td>\n      <td>0.776201</td>\n      <td>0.186624</td>\n      <td>0.580003</td>\n      <td>0.287438</td>\n      <td>-0.037945</td>\n      <td>-1.151037</td>\n      <td>-1.074581</td>\n      <td>0.307664</td>\n      <td>...</td>\n      <td>0.072024</td>\n      <td>0.011513</td>\n      <td>0.013774</td>\n      <td>-0.943706</td>\n      <td>0.441406</td>\n      <td>0.117571</td>\n      <td>0.106934</td>\n      <td>0.005371</td>\n      <td>0.452720</td>\n      <td>0.085089</td>\n    </tr>\n    <tr>\n      <th>95866</th>\n      <td>3.528371</td>\n      <td>4.667782</td>\n      <td>5.247205</td>\n      <td>4.168351</td>\n      <td>3.276534</td>\n      <td>4.309053</td>\n      <td>2.768949</td>\n      <td>3.115098</td>\n      <td>3.815794</td>\n      <td>3.940277</td>\n      <td>...</td>\n      <td>0.031871</td>\n      <td>0.012202</td>\n      <td>0.012386</td>\n      <td>1.357928</td>\n      <td>0.400391</td>\n      <td>0.135901</td>\n      <td>0.143066</td>\n      <td>0.000000</td>\n      <td>-0.795436</td>\n      <td>0.055681</td>\n    </tr>\n    <tr>\n      <th>95868</th>\n      <td>-0.033556</td>\n      <td>0.271865</td>\n      <td>0.280602</td>\n      <td>0.560396</td>\n      <td>1.511805</td>\n      <td>0.710866</td>\n      <td>1.451442</td>\n      <td>0.742910</td>\n      <td>0.739932</td>\n      <td>0.844851</td>\n      <td>...</td>\n      <td>0.055034</td>\n      <td>0.012744</td>\n      <td>0.013205</td>\n      <td>-0.223611</td>\n      <td>0.402344</td>\n      <td>0.173291</td>\n      <td>0.182617</td>\n      <td>0.000000</td>\n      <td>-0.706943</td>\n      <td>0.048163</td>\n    </tr>\n    <tr>\n      <th>95871</th>\n      <td>5.215266</td>\n      <td>4.964749</td>\n      <td>0.824476</td>\n      <td>1.733783</td>\n      <td>2.557416</td>\n      <td>3.057908</td>\n      <td>1.768616</td>\n      <td>3.665696</td>\n      <td>1.518494</td>\n      <td>1.523711</td>\n      <td>...</td>\n      <td>0.058086</td>\n      <td>0.016176</td>\n      <td>0.013788</td>\n      <td>0.166801</td>\n      <td>0.561035</td>\n      <td>0.211801</td>\n      <td>0.218262</td>\n      <td>0.000000</td>\n      <td>-0.058994</td>\n      <td>0.087301</td>\n    </tr>\n  </tbody>\n</table>\n<p>32823 rows × 518 columns</p>\n</div>"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(track_genres))\n",
    "print(len(features_df))\n",
    "features_df.isna().any()\n",
    "features_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "# generate train and test set\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features_df.iloc[:8000], track_genres.iloc[:8000], test_size=0.4, random_state=42, stratify=track_genres.iloc[:8000])\n",
    "# X_train, X_test, y_train, y_test = train_test_split(features_df, track_genres, test_size=0.4, random_state=42, stratify=track_genres)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "lab_encoder = LabelEncoder()\n",
    "lab_encoder.fit(y_train)\n",
    "\n",
    "y_train = lab_encoder.transform(y_train)\n",
    "y_test = lab_encoder.transform(y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "def evaluate_classifier(x_tr, x_te, y_tr, y_te, model):\n",
    "    model.fit(x_tr, y_tr)\n",
    "    prediction = model.predict(x_te)\n",
    "    print(prediction[:10])\n",
    "    print(y_te[:10])\n",
    "    proba_train = model.predict_proba(x_te)\n",
    "    # pd.DataFrame(proba_train, columns=list(np.unique(lab_encoder.inverse_transform(y_train))))\n",
    "    print(pd.DataFrame(proba_train))\n",
    "    print(classification_report(y_te, prediction))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h1>PCA</h1>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def pca_data(train_set, test_set, n_components, to_scale=True):\n",
    "\n",
    "    if to_scale:\n",
    "        # scale x_data\n",
    "        data_scaler = StandardScaler()\n",
    "        data_scaler.fit(train_set)\n",
    "\n",
    "        train_set = data_scaler.transform(train_set)\n",
    "        test_set = data_scaler.transform(test_set)\n",
    "\n",
    "    # PCA\n",
    "    pca = PCA(n_components = n_components)\n",
    "    pca.fit(train_set)\n",
    "\n",
    "    train_set = pca.transform(train_set)\n",
    "    test_set = pca.transform(test_set)\n",
    "\n",
    "    return train_set, test_set"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  2.60354791  -0.58995119]\n",
      " [  4.40287103  -4.76999404]\n",
      " [ 10.45776958  17.50262319]\n",
      " ...\n",
      " [  4.34920137   0.47933985]\n",
      " [ -6.16308081  -9.56565025]\n",
      " [-18.17963718   5.65331263]]\n",
      "[[16.14085012  4.24116619]\n",
      " [-3.3647734  -3.46883393]\n",
      " [-9.18277    -9.81920198]\n",
      " ...\n",
      " [ 3.71246594  6.02444555]\n",
      " [ 2.56445225  0.46266584]\n",
      " [-1.83267166 -2.24538347]]\n",
      "[[  2.6035483   -0.58996835]\n",
      " [  4.40287072  -4.76996905]\n",
      " [ 10.45777044  17.50250499]\n",
      " ...\n",
      " [  4.34920135   0.47935106]\n",
      " [ -6.16308102  -9.56561957]\n",
      " [-18.1796375    5.65333079]]\n",
      "[[16.14085019  4.24116482]\n",
      " [-3.36477334 -3.46883368]\n",
      " [-9.18277001 -9.81920519]\n",
      " ...\n",
      " [ 3.71246608  6.0244212 ]\n",
      " [ 2.56445245  0.4626638 ]\n",
      " [-1.83267153 -2.24539663]]\n"
     ]
    }
   ],
   "source": [
    "# PCA test\n",
    "x_data, x_test,_,_ = train_test_split(features_df.iloc[:8000], track_genres.iloc[:8000], test_size=0.4, random_state=42, stratify=track_genres.iloc[:8000])\n",
    "\n",
    "# already scaled\n",
    "x, y = pca_data(X_train, X_test, 2)\n",
    "print(x)\n",
    "print(y)\n",
    "\n",
    "# to scale\n",
    "x, y = pca_data(x_data, x_test, 2, to_scale=True)\n",
    "print(x)\n",
    "print(y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h1>Model parametes tuning</h1>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RepeatedStratifiedKFold, GridSearchCV\n",
    "\n",
    "\n",
    "def optimize_model(model, x_train, y_train, parameters, to_scale=False):\n",
    "    if to_scale:\n",
    "        x_train = StandardScaler().fit_transform(x_train)\n",
    "\n",
    "    #rskf = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=42)\n",
    "    #n_jobs=-1 use ll processors in parallel\n",
    "    gs = GridSearchCV(model, parameters, scoring='f1_micro', cv=2, return_train_score=True, n_jobs=-1, verbose=10)\n",
    "\n",
    "    gs.fit(x_train, y_train)\n",
    "\n",
    "    print(\"best parameters: \", gs.best_params_)\n",
    "    print(\"score: \", gs.best_score_)\n",
    "    return gs.best_estimator_\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h1>SVM</h1>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without PCA\n",
      "['Rock' 'Folk' 'Spoken' 'Rock' 'Hip-Hop' 'Rock' 'Electronic' 'Hip-Hop'\n",
      " 'Rock' 'Hip-Hop']\n",
      "track_id\n",
      "17725            Rock\n",
      "9554             Rock\n",
      "4514     Experimental\n",
      "4801             Rock\n",
      "11800         Hip-Hop\n",
      "18687            Rock\n",
      "9091       Electronic\n",
      "12369         Hip-Hop\n",
      "10149            Rock\n",
      "486              Rock\n",
      "Name: genre_top, dtype: category\n",
      "Categories (16, object): ['Blues', 'Classical', 'Country', 'Easy Listening', ..., 'Pop', 'Rock', 'Soul-RnB', 'Spoken']\n",
      "            0         1         2         3         4         5         6   \\\n",
      "0     0.000246  0.000249  0.001007  0.032155  0.126173  0.009860  0.001422   \n",
      "1     0.001262  0.000297  0.015614  0.035915  0.018611  0.086560  0.005176   \n",
      "2     0.003518  0.002004  0.031051  0.231427  0.175425  0.018743  0.052909   \n",
      "3     0.000032  0.000018  0.000165  0.015687  0.040044  0.000582  0.009684   \n",
      "4     0.000101  0.000104  0.000783  0.020025  0.002272  0.000726  0.932123   \n",
      "...        ...       ...       ...       ...       ...       ...       ...   \n",
      "3195  0.000161  0.000034  0.001999  0.259642  0.011824  0.001479  0.013465   \n",
      "3196  0.000264  0.000015  0.002347  0.007008  0.006547  0.000960  0.001341   \n",
      "3197  0.001901  0.000811  0.003278  0.010761  0.074822  0.040038  0.005659   \n",
      "3198  0.003248  0.000023  0.030550  0.001060  0.003643  0.005954  0.001007   \n",
      "3199  0.003657  0.001502  0.022763  0.162200  0.291095  0.010430  0.082347   \n",
      "\n",
      "            7         8         9         10        11        12        13  \\\n",
      "0     0.041520  0.001240  0.003454  0.000267  0.011599  0.769542  0.000266   \n",
      "1     0.001620  0.006223  0.015424  0.000133  0.077983  0.732730  0.001836   \n",
      "2     0.019367  0.049097  0.039105  0.001722  0.051007  0.115442  0.004438   \n",
      "3     0.002413  0.000428  0.003624  0.000005  0.007228  0.919907  0.000094   \n",
      "4     0.000245  0.000386  0.002216  0.000045  0.007878  0.026487  0.001568   \n",
      "...        ...       ...       ...       ...       ...       ...       ...   \n",
      "3195  0.002024  0.000578  0.000564  0.000025  0.021771  0.677423  0.008486   \n",
      "3196  0.000630  0.001847  0.008417  0.000008  0.015717  0.954499  0.000252   \n",
      "3197  0.012046  0.317395  0.009077  0.000819  0.060393  0.460815  0.000863   \n",
      "3198  0.000352  0.004123  0.002023  0.000021  0.011691  0.935075  0.000904   \n",
      "3199  0.021008  0.161104  0.096722  0.000696  0.019024  0.110659  0.003702   \n",
      "\n",
      "            14  \n",
      "0     0.001001  \n",
      "1     0.000615  \n",
      "2     0.204746  \n",
      "3     0.000087  \n",
      "4     0.005043  \n",
      "...        ...  \n",
      "3195  0.000526  \n",
      "3196  0.000148  \n",
      "3197  0.001322  \n",
      "3198  0.000326  \n",
      "3199  0.013091  \n",
      "\n",
      "[3200 rows x 15 columns]\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "              Blues       0.71      0.24      0.36        21\n",
      "          Classical       0.92      0.92      0.92        63\n",
      "            Country       0.44      0.67      0.53        36\n",
      "         Electronic       0.54      0.65      0.59       464\n",
      "       Experimental       0.55      0.56      0.55       491\n",
      "               Folk       0.52      0.76      0.61       270\n",
      "            Hip-Hop       0.59      0.79      0.67       192\n",
      "       Instrumental       0.78      0.16      0.26        45\n",
      "      International       0.56      0.69      0.62        77\n",
      "               Jazz       0.41      0.50      0.45        74\n",
      "Old-Time / Historic       0.96      0.94      0.95        81\n",
      "                Pop       0.10      0.14      0.11        79\n",
      "               Rock       0.88      0.63      0.73      1236\n",
      "           Soul-RnB       1.00      0.50      0.67        18\n",
      "             Spoken       0.48      0.72      0.58        53\n",
      "\n",
      "           accuracy                           0.63      3200\n",
      "          macro avg       0.63      0.59      0.57      3200\n",
      "       weighted avg       0.68      0.63      0.64      3200\n",
      "\n",
      "With PCA\n",
      "[[  2.60354843  -0.5899446 ]\n",
      " [  4.40287059  -4.76999516]\n",
      " [ 10.45777152  17.50257077]\n",
      " ...\n",
      " [  4.34920128   0.47934745]\n",
      " [ -6.16308127  -9.56566005]\n",
      " [-18.17963788   5.6532979 ]] [[16.14085015  4.24116728]\n",
      " [-3.36477327 -3.46883079]\n",
      " [-9.1827701  -9.81920447]\n",
      " ...\n",
      " [ 3.71246624  6.0244501 ]\n",
      " [ 2.56445252  0.462667  ]\n",
      " [-1.83267143 -2.24538499]]\n",
      "['Instrumental' 'Spoken' 'Soul-RnB' 'Rock' 'Hip-Hop' 'Rock' 'Electronic'\n",
      " 'Hip-Hop' 'Rock' 'Rock']\n",
      "track_id\n",
      "17725            Rock\n",
      "9554             Rock\n",
      "4514     Experimental\n",
      "4801             Rock\n",
      "11800         Hip-Hop\n",
      "18687            Rock\n",
      "9091       Electronic\n",
      "12369         Hip-Hop\n",
      "10149            Rock\n",
      "486              Rock\n",
      "Name: genre_top, dtype: category\n",
      "Categories (16, object): ['Blues', 'Classical', 'Country', 'Easy Listening', ..., 'Pop', 'Rock', 'Soul-RnB', 'Spoken']\n",
      "            0         1         2         3         4         5         6   \\\n",
      "0     0.001402  0.002826  0.002588  0.119894  0.264774  0.013905  0.014046   \n",
      "1     0.010005  0.003428  0.018245  0.146892  0.168706  0.026813  0.144749   \n",
      "2     0.003743  0.002304  0.005481  0.399990  0.193272  0.012475  0.075202   \n",
      "3     0.001193  0.001183  0.005024  0.057890  0.113393  0.007099  0.010324   \n",
      "4     0.003115  0.001567  0.006663  0.215256  0.104752  0.008722  0.197019   \n",
      "...        ...       ...       ...       ...       ...       ...       ...   \n",
      "3195  0.001345  0.001640  0.003492  0.346892  0.128794  0.009195  0.146760   \n",
      "3196  0.002840  0.000949  0.011673  0.057300  0.084837  0.008884  0.019079   \n",
      "3197  0.004487  0.002643  0.005759  0.067152  0.234555  0.088493  0.009370   \n",
      "3198  0.005915  0.001175  0.014955  0.049741  0.094396  0.017340  0.020062   \n",
      "3199  0.010513  0.002703  0.020299  0.099797  0.151279  0.027309  0.089090   \n",
      "\n",
      "            7         8         9         10        11        12        13  \\\n",
      "0     0.032991  0.006217  0.011470  0.001608  0.015409  0.506029  0.002083   \n",
      "1     0.009424  0.023451  0.044565  0.000255  0.038385  0.313958  0.008781   \n",
      "2     0.015879  0.030081  0.025052  0.000852  0.017572  0.166302  0.016561   \n",
      "3     0.014099  0.002367  0.008720  0.000425  0.020427  0.748523  0.004525   \n",
      "4     0.007203  0.004458  0.012867  0.000355  0.026183  0.384785  0.011631   \n",
      "...        ...       ...       ...       ...       ...       ...       ...   \n",
      "3195  0.009188  0.002733  0.008637  0.000696  0.019451  0.304895  0.007518   \n",
      "3196  0.010597  0.003712  0.011106  0.000232  0.024706  0.751051  0.005585   \n",
      "3197  0.026001  0.021529  0.006245  0.001701  0.021273  0.502689  0.002643   \n",
      "3198  0.011293  0.009699  0.012915  0.000262  0.027316  0.719506  0.004525   \n",
      "3199  0.009872  0.020358  0.032764  0.000254  0.039131  0.455962  0.006947   \n",
      "\n",
      "            14  \n",
      "0     0.004758  \n",
      "1     0.042343  \n",
      "2     0.035234  \n",
      "3     0.004808  \n",
      "4     0.015425  \n",
      "...        ...  \n",
      "3195  0.008764  \n",
      "3196  0.007449  \n",
      "3197  0.005461  \n",
      "3198  0.010898  \n",
      "3199  0.033719  \n",
      "\n",
      "[3200 rows x 15 columns]\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "              Blues       0.01      0.24      0.02        21\n",
      "          Classical       0.23      0.56      0.32        63\n",
      "            Country       0.05      0.14      0.08        36\n",
      "         Electronic       0.53      0.21      0.30       464\n",
      "       Experimental       0.37      0.07      0.11       491\n",
      "               Folk       0.36      0.25      0.30       270\n",
      "            Hip-Hop       0.19      0.53      0.28       192\n",
      "       Instrumental       0.03      0.09      0.05        45\n",
      "      International       0.08      0.03      0.04        77\n",
      "               Jazz       0.12      0.24      0.16        74\n",
      "Old-Time / Historic       0.33      0.64      0.43        81\n",
      "                Pop       0.00      0.00      0.00        79\n",
      "               Rock       0.71      0.49      0.58      1236\n",
      "           Soul-RnB       0.00      0.00      0.00        18\n",
      "             Spoken       0.03      0.09      0.05        53\n",
      "\n",
      "           accuracy                           0.32      3200\n",
      "          macro avg       0.20      0.24      0.18      3200\n",
      "       weighted avg       0.47      0.32      0.35      3200\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marco/repo/ML/venv/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/marco/repo/ML/venv/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/marco/repo/ML/venv/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm_model = SVC(kernel='rbf', class_weight='balanced', probability=True)\n",
    "print(\"Without PCA\")\n",
    "evaluate_classifier(X_train, X_test, y_train, y_test, svm_model)\n",
    "\n",
    "pca_svm_model = SVC(kernel='rbf', class_weight='balanced', probability=True)\n",
    "print(\"With PCA\")\n",
    "pca_train, pca_test = pca_data(X_train, X_test, 2)\n",
    "print(pca_train, pca_test)\n",
    "evaluate_classifier(pca_train, pca_test, y_train, y_test, pca_svm_model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 16 candidates, totalling 32 fits\n",
      "[CV 1/2; 1/16] START C=0.1, gamma=1.............................................\n",
      "[CV 1/2; 1/16] END C=0.1, gamma=1;, score=(train=0.097, test=0.084) total time=  22.0s\n",
      "[CV 2/2; 1/16] START C=0.1, gamma=1.............................................\n",
      "[CV 2/2; 1/16] END C=0.1, gamma=1;, score=(train=0.120, test=0.084) total time=  21.1s\n",
      "[CV 1/2; 2/16] START C=0.1, gamma=0.1...........................................\n",
      "[CV 1/2; 2/16] END C=0.1, gamma=0.1;, score=(train=0.097, test=0.084) total time=  19.2s\n",
      "[CV 2/2; 2/16] START C=0.1, gamma=0.1...........................................\n",
      "[CV 2/2; 2/16] END C=0.1, gamma=0.1;, score=(train=0.120, test=0.084) total time=  19.8s\n",
      "[CV 1/2; 3/16] START C=0.1, gamma=0.01..........................................\n",
      "[CV 1/2; 3/16] END C=0.1, gamma=0.01;, score=(train=0.108, test=0.085) total time=  19.4s\n",
      "[CV 2/2; 3/16] START C=0.1, gamma=0.01..........................................\n",
      "[CV 2/2; 3/16] END C=0.1, gamma=0.01;, score=(train=0.128, test=0.088) total time=  22.6s\n",
      "[CV 1/2; 4/16] START C=0.1, gamma=0.001.........................................\n",
      "[CV 1/2; 4/16] END C=0.1, gamma=0.001;, score=(train=0.468, test=0.452) total time=  19.0s\n",
      "[CV 2/2; 4/16] START C=0.1, gamma=0.001.........................................\n",
      "[CV 2/2; 4/16] END C=0.1, gamma=0.001;, score=(train=0.483, test=0.420) total time=  19.1s\n",
      "[CV 1/2; 5/16] START C=1, gamma=1...............................................\n",
      "[CV 1/2; 5/16] END C=1, gamma=1;, score=(train=0.478, test=0.158) total time=  23.3s\n",
      "[CV 2/2; 5/16] START C=1, gamma=1...............................................\n",
      "[CV 2/2; 5/16] END C=1, gamma=1;, score=(train=0.627, test=0.390) total time=  25.4s\n",
      "[CV 1/2; 6/16] START C=1, gamma=0.1.............................................\n",
      "[CV 1/2; 6/16] END C=1, gamma=0.1;, score=(train=0.478, test=0.158) total time=  22.8s\n",
      "[CV 2/2; 6/16] START C=1, gamma=0.1.............................................\n",
      "[CV 2/2; 6/16] END C=1, gamma=0.1;, score=(train=0.627, test=0.390) total time=  20.6s\n",
      "[CV 1/2; 7/16] START C=1, gamma=0.01............................................\n",
      "[CV 1/2; 7/16] END C=1, gamma=0.01;, score=(train=0.807, test=0.425) total time=  17.9s\n",
      "[CV 2/2; 7/16] START C=1, gamma=0.01............................................\n",
      "[CV 2/2; 7/16] END C=1, gamma=0.01;, score=(train=0.818, test=0.426) total time=  18.6s\n",
      "[CV 1/2; 8/16] START C=1, gamma=0.001...........................................\n",
      "[CV 1/2; 8/16] END C=1, gamma=0.001;, score=(train=0.692, test=0.593) total time=  12.6s\n",
      "[CV 2/2; 8/16] START C=1, gamma=0.001...........................................\n",
      "[CV 2/2; 8/16] END C=1, gamma=0.001;, score=(train=0.713, test=0.556) total time=  12.6s\n",
      "[CV 1/2; 9/16] START C=10, gamma=1..............................................\n",
      "[CV 1/2; 9/16] END C=10, gamma=1;, score=(train=1.000, test=0.397) total time=  21.9s\n",
      "[CV 2/2; 9/16] START C=10, gamma=1..............................................\n",
      "[CV 2/2; 9/16] END C=10, gamma=1;, score=(train=1.000, test=0.396) total time=  21.5s\n",
      "[CV 1/2; 10/16] START C=10, gamma=0.1...........................................\n",
      "[CV 1/2; 10/16] END C=10, gamma=0.1;, score=(train=1.000, test=0.397) total time=  21.1s\n",
      "[CV 2/2; 10/16] START C=10, gamma=0.1...........................................\n",
      "[CV 2/2; 10/16] END C=10, gamma=0.1;, score=(train=1.000, test=0.396) total time=  19.8s\n",
      "[CV 1/2; 11/16] START C=10, gamma=0.01..........................................\n",
      "[CV 1/2; 11/16] END C=10, gamma=0.01;, score=(train=1.000, test=0.492) total time=  17.8s\n",
      "[CV 2/2; 11/16] START C=10, gamma=0.01..........................................\n",
      "[CV 2/2; 11/16] END C=10, gamma=0.01;, score=(train=1.000, test=0.494) total time=  18.0s\n",
      "[CV 1/2; 12/16] START C=10, gamma=0.001.........................................\n",
      "[CV 1/2; 12/16] END C=10, gamma=0.001;, score=(train=0.933, test=0.677) total time=   8.2s\n",
      "[CV 2/2; 12/16] START C=10, gamma=0.001.........................................\n",
      "[CV 2/2; 12/16] END C=10, gamma=0.001;, score=(train=0.937, test=0.654) total time=   7.9s\n",
      "[CV 1/2; 13/16] START C=100, gamma=1............................................\n",
      "[CV 1/2; 13/16] END C=100, gamma=1;, score=(train=1.000, test=0.397) total time=  21.6s\n",
      "[CV 2/2; 13/16] START C=100, gamma=1............................................\n",
      "[CV 2/2; 13/16] END C=100, gamma=1;, score=(train=1.000, test=0.396) total time=  21.8s\n",
      "[CV 1/2; 14/16] START C=100, gamma=0.1..........................................\n",
      "[CV 1/2; 14/16] END C=100, gamma=0.1;, score=(train=1.000, test=0.397) total time=  19.8s\n",
      "[CV 2/2; 14/16] START C=100, gamma=0.1..........................................\n",
      "[CV 2/2; 14/16] END C=100, gamma=0.1;, score=(train=1.000, test=0.396) total time=  19.8s\n",
      "[CV 1/2; 15/16] START C=100, gamma=0.01.........................................\n",
      "[CV 1/2; 15/16] END C=100, gamma=0.01;, score=(train=1.000, test=0.492) total time=  17.8s\n",
      "[CV 2/2; 15/16] START C=100, gamma=0.01.........................................\n",
      "[CV 2/2; 15/16] END C=100, gamma=0.01;, score=(train=1.000, test=0.494) total time=  18.1s\n",
      "[CV 1/2; 16/16] START C=100, gamma=0.001........................................\n",
      "[CV 1/2; 16/16] END C=100, gamma=0.001;, score=(train=1.000, test=0.672) total time=   8.2s\n",
      "[CV 2/2; 16/16] START C=100, gamma=0.001........................................\n",
      "[CV 2/2; 16/16] END C=100, gamma=0.001;, score=(train=1.000, test=0.661) total time=   7.7s\n",
      "best parameters:  {'C': 100, 'gamma': 0.001}\n",
      "score:  0.66625\n",
      "['Rock' 'Rock' 'Electronic' 'Rock' 'Hip-Hop' 'Rock' 'Electronic' 'Hip-Hop'\n",
      " 'Rock' 'Rock']\n",
      "track_id\n",
      "17725            Rock\n",
      "9554             Rock\n",
      "4514     Experimental\n",
      "4801             Rock\n",
      "11800         Hip-Hop\n",
      "18687            Rock\n",
      "9091       Electronic\n",
      "12369         Hip-Hop\n",
      "10149            Rock\n",
      "486              Rock\n",
      "Name: genre_top, dtype: category\n",
      "Categories (16, object): ['Blues', 'Classical', 'Country', 'Easy Listening', ..., 'Pop', 'Rock', 'Soul-RnB', 'Spoken']\n",
      "            0         1         2         3         4         5         6   \\\n",
      "0     0.000167  0.000104  0.001194  0.006652  0.107512  0.008175  0.000820   \n",
      "1     0.000657  0.000249  0.008991  0.012964  0.044693  0.280048  0.002606   \n",
      "2     0.004767  0.003087  0.027891  0.273024  0.184265  0.046842  0.033813   \n",
      "3     0.000046  0.000041  0.000160  0.011139  0.027602  0.000714  0.009123   \n",
      "4     0.000207  0.000172  0.000853  0.009971  0.004916  0.001422  0.952711   \n",
      "...        ...       ...       ...       ...       ...       ...       ...   \n",
      "3195  0.000330  0.000128  0.001975  0.269771  0.015678  0.005085  0.030802   \n",
      "3196  0.000368  0.000040  0.001351  0.007375  0.004173  0.001414  0.001120   \n",
      "3197  0.004397  0.002098  0.009763  0.027079  0.095105  0.066298  0.018302   \n",
      "3198  0.002090  0.000023  0.008275  0.001086  0.003297  0.009361  0.001797   \n",
      "3199  0.003742  0.001836  0.013960  0.451144  0.169880  0.014364  0.084870   \n",
      "\n",
      "            7         8         9         10        11        12        13  \\\n",
      "0     0.024069  0.000863  0.002719  0.000142  0.020541  0.826322  0.000167   \n",
      "1     0.002273  0.006392  0.011225  0.000104  0.023651  0.604188  0.001469   \n",
      "2     0.016186  0.035782  0.031262  0.002708  0.063531  0.112606  0.005532   \n",
      "3     0.003511  0.000446  0.005401  0.000017  0.004648  0.936937  0.000139   \n",
      "4     0.000265  0.000334  0.003750  0.000061  0.005603  0.010075  0.002268   \n",
      "...        ...       ...       ...       ...       ...       ...       ...   \n",
      "3195  0.002119  0.000837  0.000645  0.000102  0.023640  0.636188  0.012113   \n",
      "3196  0.000553  0.002277  0.003902  0.000035  0.011375  0.965685  0.000206   \n",
      "3197  0.017659  0.509063  0.024938  0.002235  0.100818  0.117100  0.002168   \n",
      "3198  0.000315  0.004249  0.001709  0.000029  0.005439  0.961387  0.000751   \n",
      "3199  0.014322  0.104178  0.049903  0.000933  0.016234  0.062172  0.003692   \n",
      "\n",
      "            14  \n",
      "0     0.000553  \n",
      "1     0.000489  \n",
      "2     0.158705  \n",
      "3     0.000076  \n",
      "4     0.007393  \n",
      "...        ...  \n",
      "3195  0.000587  \n",
      "3196  0.000125  \n",
      "3197  0.002977  \n",
      "3198  0.000192  \n",
      "3199  0.008770  \n",
      "\n",
      "[3200 rows x 15 columns]\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "              Blues       0.86      0.29      0.43        21\n",
      "          Classical       0.92      0.90      0.91        63\n",
      "            Country       0.75      0.58      0.66        36\n",
      "         Electronic       0.57      0.67      0.61       464\n",
      "       Experimental       0.57      0.57      0.57       491\n",
      "               Folk       0.64      0.68      0.66       270\n",
      "            Hip-Hop       0.74      0.71      0.73       192\n",
      "       Instrumental       0.73      0.18      0.29        45\n",
      "      International       0.81      0.57      0.67        77\n",
      "               Jazz       0.65      0.47      0.55        74\n",
      "Old-Time / Historic       0.97      0.96      0.97        81\n",
      "                Pop       0.28      0.09      0.13        79\n",
      "               Rock       0.78      0.82      0.80      1236\n",
      "           Soul-RnB       1.00      0.50      0.67        18\n",
      "             Spoken       0.62      0.64      0.63        53\n",
      "\n",
      "           accuracy                           0.70      3200\n",
      "          macro avg       0.73      0.58      0.62      3200\n",
      "       weighted avg       0.69      0.70      0.69      3200\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "SVC(C=100, class_weight='balanced', gamma=0.001, probability=True)",
      "text/html": "<style>#sk-container-id-10 {color: black;background-color: white;}#sk-container-id-10 pre{padding: 0;}#sk-container-id-10 div.sk-toggleable {background-color: white;}#sk-container-id-10 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-10 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-10 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-10 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-10 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-10 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-10 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-10 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-10 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-10 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-10 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-10 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-10 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-10 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-10 div.sk-item {position: relative;z-index: 1;}#sk-container-id-10 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-10 div.sk-item::before, #sk-container-id-10 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-10 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-10 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-10 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-10 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-10 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-10 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-10 div.sk-label-container {text-align: center;}#sk-container-id-10 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-10 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-10\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(C=100, class_weight=&#x27;balanced&#x27;, gamma=0.001, probability=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" checked><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(C=100, class_weight=&#x27;balanced&#x27;, gamma=0.001, probability=True)</pre></div></div></div></div></div>"
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SVM model evaluation\n",
    "param_evaluated = {'C': [0.1,1, 10, 100], 'gamma': [1,0.1,0.01,0.001]}\n",
    "\n",
    "svm_base_model = SVC(kernel='rbf', class_weight='balanced', probability=True)\n",
    "svm_best_model = optimize_model(svm_base_model, X_train, y_train, param_evaluated)\n",
    "evaluate_classifier(X_train, X_test, y_train, y_test,svm_best_model)\n",
    "svm_best_model\n",
    "\n",
    "# After run\n",
    "# best parameters:  {'C': 100, 'gamma': 0.001}\n",
    "# score:  0.66625"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h1>K-nearest neighbors</h1>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Rock' 'Rock' 'Country' 'Rock' 'Electronic' 'Rock' 'Electronic' 'Hip-Hop'\n",
      " 'Rock' 'Hip-Hop']\n",
      "track_id\n",
      "17725            Rock\n",
      "9554             Rock\n",
      "4514     Experimental\n",
      "4801             Rock\n",
      "11800         Hip-Hop\n",
      "18687            Rock\n",
      "9091       Electronic\n",
      "12369         Hip-Hop\n",
      "10149            Rock\n",
      "486              Rock\n",
      "Name: genre_top, dtype: category\n",
      "Categories (16, object): ['Blues', 'Classical', 'Country', 'Easy Listening', ..., 'Pop', 'Rock', 'Soul-RnB', 'Spoken']\n",
      "       0    1    2    3    4    5    6    7    8    9    10   11   12   13  \\\n",
      "0     0.0  0.0  0.0  0.0  0.2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.8  0.0   \n",
      "1     0.0  0.0  0.0  0.0  0.0  0.2  0.0  0.0  0.2  0.0  0.0  0.0  0.6  0.0   \n",
      "2     0.0  0.0  0.2  0.0  0.0  0.0  0.2  0.0  0.2  0.0  0.0  0.2  0.0  0.0   \n",
      "3     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0   \n",
      "4     0.0  0.0  0.0  0.4  0.0  0.0  0.4  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "...   ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
      "3195  0.0  0.0  0.0  0.0  0.0  0.2  0.0  0.0  0.0  0.0  0.0  0.0  0.6  0.2   \n",
      "3196  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0   \n",
      "3197  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.4  0.0  0.0  0.0  0.6  0.0   \n",
      "3198  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.2  0.0  0.0  0.0  0.8  0.0   \n",
      "3199  0.0  0.0  0.0  0.0  0.2  0.0  0.2  0.0  0.2  0.2  0.0  0.0  0.2  0.0   \n",
      "\n",
      "       14  \n",
      "0     0.0  \n",
      "1     0.0  \n",
      "2     0.2  \n",
      "3     0.0  \n",
      "4     0.2  \n",
      "...   ...  \n",
      "3195  0.0  \n",
      "3196  0.0  \n",
      "3197  0.0  \n",
      "3198  0.0  \n",
      "3199  0.0  \n",
      "\n",
      "[3200 rows x 15 columns]\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "              Blues       0.38      0.29      0.32        21\n",
      "          Classical       0.57      0.95      0.71        63\n",
      "            Country       0.23      0.44      0.30        36\n",
      "         Electronic       0.58      0.38      0.46       464\n",
      "       Experimental       0.56      0.43      0.49       491\n",
      "               Folk       0.56      0.47      0.51       270\n",
      "            Hip-Hop       0.62      0.57      0.59       192\n",
      "       Instrumental       0.50      0.02      0.04        45\n",
      "      International       0.40      0.55      0.46        77\n",
      "               Jazz       0.48      0.30      0.37        74\n",
      "Old-Time / Historic       0.95      0.91      0.93        81\n",
      "                Pop       0.31      0.06      0.11        79\n",
      "               Rock       0.65      0.86      0.74      1236\n",
      "           Soul-RnB       0.56      0.28      0.37        18\n",
      "             Spoken       0.61      0.43      0.51        53\n",
      "\n",
      "           accuracy                           0.61      3200\n",
      "          macro avg       0.53      0.46      0.46      3200\n",
      "       weighted avg       0.60      0.61      0.58      3200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn_model = KNeighborsClassifier()\n",
    "evaluate_classifier(X_train, X_test, y_train, y_test, knn_model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 162 candidates, totalling 324 fits\n",
      "[CV 1/2; 1/162] START leaf_size=1, n_neighbors=1, p=1...........................\n",
      "[CV 1/2; 1/162] END leaf_size=1, n_neighbors=1, p=1;, score=(train=1.000, test=0.575) total time=   0.9s\n",
      "[CV 2/2; 1/162] START leaf_size=1, n_neighbors=1, p=1...........................\n",
      "[CV 2/2; 1/162] END leaf_size=1, n_neighbors=1, p=1;, score=(train=1.000, test=0.571) total time=   0.6s\n",
      "[CV 1/2; 2/162] START leaf_size=1, n_neighbors=1, p=2...........................\n",
      "[CV 1/2; 2/162] END leaf_size=1, n_neighbors=1, p=2;, score=(train=1.000, test=0.560) total time=   0.2s\n",
      "[CV 2/2; 2/162] START leaf_size=1, n_neighbors=1, p=2...........................\n",
      "[CV 2/2; 2/162] END leaf_size=1, n_neighbors=1, p=2;, score=(train=1.000, test=0.552) total time=   0.3s\n",
      "[CV 1/2; 3/162] START leaf_size=1, n_neighbors=2, p=1...........................\n",
      "[CV 1/2; 3/162] END leaf_size=1, n_neighbors=2, p=1;, score=(train=0.827, test=0.539) total time=   1.1s\n",
      "[CV 2/2; 3/162] START leaf_size=1, n_neighbors=2, p=1...........................\n",
      "[CV 2/2; 3/162] END leaf_size=1, n_neighbors=2, p=1;, score=(train=0.840, test=0.539) total time=   0.6s\n",
      "[CV 1/2; 4/162] START leaf_size=1, n_neighbors=2, p=2...........................\n",
      "[CV 1/2; 4/162] END leaf_size=1, n_neighbors=2, p=2;, score=(train=0.825, test=0.531) total time=   0.2s\n",
      "[CV 2/2; 4/162] START leaf_size=1, n_neighbors=2, p=2...........................\n",
      "[CV 2/2; 4/162] END leaf_size=1, n_neighbors=2, p=2;, score=(train=0.840, test=0.504) total time=   0.2s\n",
      "[CV 1/2; 5/162] START leaf_size=1, n_neighbors=3, p=1...........................\n",
      "[CV 1/2; 5/162] END leaf_size=1, n_neighbors=3, p=1;, score=(train=0.762, test=0.574) total time=   0.6s\n",
      "[CV 2/2; 5/162] START leaf_size=1, n_neighbors=3, p=1...........................\n",
      "[CV 2/2; 5/162] END leaf_size=1, n_neighbors=3, p=1;, score=(train=0.784, test=0.569) total time=   0.6s\n",
      "[CV 1/2; 6/162] START leaf_size=1, n_neighbors=3, p=2...........................\n",
      "[CV 1/2; 6/162] END leaf_size=1, n_neighbors=3, p=2;, score=(train=0.756, test=0.560) total time=   0.2s\n",
      "[CV 2/2; 6/162] START leaf_size=1, n_neighbors=3, p=2...........................\n",
      "[CV 2/2; 6/162] END leaf_size=1, n_neighbors=3, p=2;, score=(train=0.783, test=0.555) total time=   0.2s\n",
      "[CV 1/2; 7/162] START leaf_size=1, n_neighbors=4, p=1...........................\n",
      "[CV 1/2; 7/162] END leaf_size=1, n_neighbors=4, p=1;, score=(train=0.736, test=0.593) total time=   0.6s\n",
      "[CV 2/2; 7/162] START leaf_size=1, n_neighbors=4, p=1...........................\n",
      "[CV 2/2; 7/162] END leaf_size=1, n_neighbors=4, p=1;, score=(train=0.753, test=0.578) total time=   0.5s\n",
      "[CV 1/2; 8/162] START leaf_size=1, n_neighbors=4, p=2...........................\n",
      "[CV 1/2; 8/162] END leaf_size=1, n_neighbors=4, p=2;, score=(train=0.723, test=0.578) total time=   0.2s\n",
      "[CV 2/2; 8/162] START leaf_size=1, n_neighbors=4, p=2...........................\n",
      "[CV 2/2; 8/162] END leaf_size=1, n_neighbors=4, p=2;, score=(train=0.740, test=0.566) total time=   0.2s\n",
      "[CV 1/2; 9/162] START leaf_size=1, n_neighbors=5, p=1...........................\n",
      "[CV 1/2; 9/162] END leaf_size=1, n_neighbors=5, p=1;, score=(train=0.709, test=0.585) total time=   0.6s\n",
      "[CV 2/2; 9/162] START leaf_size=1, n_neighbors=5, p=1...........................\n",
      "[CV 2/2; 9/162] END leaf_size=1, n_neighbors=5, p=1;, score=(train=0.730, test=0.575) total time=   0.6s\n",
      "[CV 1/2; 10/162] START leaf_size=1, n_neighbors=5, p=2..........................\n",
      "[CV 1/2; 10/162] END leaf_size=1, n_neighbors=5, p=2;, score=(train=0.698, test=0.575) total time=   0.2s\n",
      "[CV 2/2; 10/162] START leaf_size=1, n_neighbors=5, p=2..........................\n",
      "[CV 2/2; 10/162] END leaf_size=1, n_neighbors=5, p=2;, score=(train=0.713, test=0.565) total time=   0.2s\n",
      "[CV 1/2; 11/162] START leaf_size=1, n_neighbors=6, p=1..........................\n",
      "[CV 1/2; 11/162] END leaf_size=1, n_neighbors=6, p=1;, score=(train=0.689, test=0.595) total time=   0.6s\n",
      "[CV 2/2; 11/162] START leaf_size=1, n_neighbors=6, p=1..........................\n",
      "[CV 2/2; 11/162] END leaf_size=1, n_neighbors=6, p=1;, score=(train=0.705, test=0.569) total time=   0.5s\n",
      "[CV 1/2; 12/162] START leaf_size=1, n_neighbors=6, p=2..........................\n",
      "[CV 1/2; 12/162] END leaf_size=1, n_neighbors=6, p=2;, score=(train=0.680, test=0.568) total time=   0.2s\n",
      "[CV 2/2; 12/162] START leaf_size=1, n_neighbors=6, p=2..........................\n",
      "[CV 2/2; 12/162] END leaf_size=1, n_neighbors=6, p=2;, score=(train=0.705, test=0.569) total time=   0.2s\n",
      "[CV 1/2; 13/162] START leaf_size=1, n_neighbors=7, p=1..........................\n",
      "[CV 1/2; 13/162] END leaf_size=1, n_neighbors=7, p=1;, score=(train=0.667, test=0.587) total time=   0.6s\n",
      "[CV 2/2; 13/162] START leaf_size=1, n_neighbors=7, p=1..........................\n",
      "[CV 2/2; 13/162] END leaf_size=1, n_neighbors=7, p=1;, score=(train=0.698, test=0.578) total time=   0.6s\n",
      "[CV 1/2; 14/162] START leaf_size=1, n_neighbors=7, p=2..........................\n",
      "[CV 1/2; 14/162] END leaf_size=1, n_neighbors=7, p=2;, score=(train=0.659, test=0.570) total time=   0.2s\n",
      "[CV 2/2; 14/162] START leaf_size=1, n_neighbors=7, p=2..........................\n",
      "[CV 2/2; 14/162] END leaf_size=1, n_neighbors=7, p=2;, score=(train=0.685, test=0.561) total time=   0.2s\n",
      "[CV 1/2; 15/162] START leaf_size=1, n_neighbors=8, p=1..........................\n",
      "[CV 1/2; 15/162] END leaf_size=1, n_neighbors=8, p=1;, score=(train=0.665, test=0.586) total time=   0.6s\n",
      "[CV 2/2; 15/162] START leaf_size=1, n_neighbors=8, p=1..........................\n",
      "[CV 2/2; 15/162] END leaf_size=1, n_neighbors=8, p=1;, score=(train=0.683, test=0.578) total time=   0.5s\n",
      "[CV 1/2; 16/162] START leaf_size=1, n_neighbors=8, p=2..........................\n",
      "[CV 1/2; 16/162] END leaf_size=1, n_neighbors=8, p=2;, score=(train=0.643, test=0.574) total time=   0.2s\n",
      "[CV 2/2; 16/162] START leaf_size=1, n_neighbors=8, p=2..........................\n",
      "[CV 2/2; 16/162] END leaf_size=1, n_neighbors=8, p=2;, score=(train=0.673, test=0.566) total time=   0.3s\n",
      "[CV 1/2; 17/162] START leaf_size=1, n_neighbors=9, p=1..........................\n",
      "[CV 1/2; 17/162] END leaf_size=1, n_neighbors=9, p=1;, score=(train=0.651, test=0.583) total time=   0.6s\n",
      "[CV 2/2; 17/162] START leaf_size=1, n_neighbors=9, p=1..........................\n",
      "[CV 2/2; 17/162] END leaf_size=1, n_neighbors=9, p=1;, score=(train=0.675, test=0.575) total time=   0.6s\n",
      "[CV 1/2; 18/162] START leaf_size=1, n_neighbors=9, p=2..........................\n",
      "[CV 1/2; 18/162] END leaf_size=1, n_neighbors=9, p=2;, score=(train=0.632, test=0.568) total time=   0.2s\n",
      "[CV 2/2; 18/162] START leaf_size=1, n_neighbors=9, p=2..........................\n",
      "[CV 2/2; 18/162] END leaf_size=1, n_neighbors=9, p=2;, score=(train=0.652, test=0.565) total time=   0.2s\n",
      "[CV 1/2; 19/162] START leaf_size=2, n_neighbors=1, p=1..........................\n",
      "[CV 1/2; 19/162] END leaf_size=2, n_neighbors=1, p=1;, score=(train=1.000, test=0.575) total time=   0.6s\n",
      "[CV 2/2; 19/162] START leaf_size=2, n_neighbors=1, p=1..........................\n",
      "[CV 2/2; 19/162] END leaf_size=2, n_neighbors=1, p=1;, score=(train=1.000, test=0.571) total time=   0.6s\n",
      "[CV 1/2; 20/162] START leaf_size=2, n_neighbors=1, p=2..........................\n",
      "[CV 1/2; 20/162] END leaf_size=2, n_neighbors=1, p=2;, score=(train=1.000, test=0.560) total time=   0.1s\n",
      "[CV 2/2; 20/162] START leaf_size=2, n_neighbors=1, p=2..........................\n",
      "[CV 2/2; 20/162] END leaf_size=2, n_neighbors=1, p=2;, score=(train=1.000, test=0.552) total time=   0.2s\n",
      "[CV 1/2; 21/162] START leaf_size=2, n_neighbors=2, p=1..........................\n",
      "[CV 1/2; 21/162] END leaf_size=2, n_neighbors=2, p=1;, score=(train=0.827, test=0.539) total time=   0.6s\n",
      "[CV 2/2; 21/162] START leaf_size=2, n_neighbors=2, p=1..........................\n",
      "[CV 2/2; 21/162] END leaf_size=2, n_neighbors=2, p=1;, score=(train=0.840, test=0.539) total time=   0.7s\n",
      "[CV 1/2; 22/162] START leaf_size=2, n_neighbors=2, p=2..........................\n",
      "[CV 1/2; 22/162] END leaf_size=2, n_neighbors=2, p=2;, score=(train=0.825, test=0.531) total time=   0.2s\n",
      "[CV 2/2; 22/162] START leaf_size=2, n_neighbors=2, p=2..........................\n",
      "[CV 2/2; 22/162] END leaf_size=2, n_neighbors=2, p=2;, score=(train=0.840, test=0.504) total time=   0.2s\n",
      "[CV 1/2; 23/162] START leaf_size=2, n_neighbors=3, p=1..........................\n",
      "[CV 1/2; 23/162] END leaf_size=2, n_neighbors=3, p=1;, score=(train=0.762, test=0.574) total time=   0.6s\n",
      "[CV 2/2; 23/162] START leaf_size=2, n_neighbors=3, p=1..........................\n",
      "[CV 2/2; 23/162] END leaf_size=2, n_neighbors=3, p=1;, score=(train=0.784, test=0.569) total time=   0.6s\n",
      "[CV 1/2; 24/162] START leaf_size=2, n_neighbors=3, p=2..........................\n",
      "[CV 1/2; 24/162] END leaf_size=2, n_neighbors=3, p=2;, score=(train=0.756, test=0.560) total time=   0.2s\n",
      "[CV 2/2; 24/162] START leaf_size=2, n_neighbors=3, p=2..........................\n",
      "[CV 2/2; 24/162] END leaf_size=2, n_neighbors=3, p=2;, score=(train=0.783, test=0.555) total time=   0.2s\n",
      "[CV 1/2; 25/162] START leaf_size=2, n_neighbors=4, p=1..........................\n",
      "[CV 1/2; 25/162] END leaf_size=2, n_neighbors=4, p=1;, score=(train=0.736, test=0.593) total time=   0.6s\n",
      "[CV 2/2; 25/162] START leaf_size=2, n_neighbors=4, p=1..........................\n",
      "[CV 2/2; 25/162] END leaf_size=2, n_neighbors=4, p=1;, score=(train=0.753, test=0.578) total time=   0.5s\n",
      "[CV 1/2; 26/162] START leaf_size=2, n_neighbors=4, p=2..........................\n",
      "[CV 1/2; 26/162] END leaf_size=2, n_neighbors=4, p=2;, score=(train=0.723, test=0.578) total time=   0.2s\n",
      "[CV 2/2; 26/162] START leaf_size=2, n_neighbors=4, p=2..........................\n",
      "[CV 2/2; 26/162] END leaf_size=2, n_neighbors=4, p=2;, score=(train=0.740, test=0.566) total time=   0.2s\n",
      "[CV 1/2; 27/162] START leaf_size=2, n_neighbors=5, p=1..........................\n",
      "[CV 1/2; 27/162] END leaf_size=2, n_neighbors=5, p=1;, score=(train=0.709, test=0.585) total time=   0.5s\n",
      "[CV 2/2; 27/162] START leaf_size=2, n_neighbors=5, p=1..........................\n",
      "[CV 2/2; 27/162] END leaf_size=2, n_neighbors=5, p=1;, score=(train=0.730, test=0.575) total time=   0.6s\n",
      "[CV 1/2; 28/162] START leaf_size=2, n_neighbors=5, p=2..........................\n",
      "[CV 1/2; 28/162] END leaf_size=2, n_neighbors=5, p=2;, score=(train=0.698, test=0.575) total time=   0.2s\n",
      "[CV 2/2; 28/162] START leaf_size=2, n_neighbors=5, p=2..........................\n",
      "[CV 2/2; 28/162] END leaf_size=2, n_neighbors=5, p=2;, score=(train=0.713, test=0.565) total time=   0.2s\n",
      "[CV 1/2; 29/162] START leaf_size=2, n_neighbors=6, p=1..........................\n",
      "[CV 1/2; 29/162] END leaf_size=2, n_neighbors=6, p=1;, score=(train=0.689, test=0.595) total time=   0.6s\n",
      "[CV 2/2; 29/162] START leaf_size=2, n_neighbors=6, p=1..........................\n",
      "[CV 2/2; 29/162] END leaf_size=2, n_neighbors=6, p=1;, score=(train=0.705, test=0.569) total time=   0.5s\n",
      "[CV 1/2; 30/162] START leaf_size=2, n_neighbors=6, p=2..........................\n",
      "[CV 1/2; 30/162] END leaf_size=2, n_neighbors=6, p=2;, score=(train=0.680, test=0.568) total time=   0.2s\n",
      "[CV 2/2; 30/162] START leaf_size=2, n_neighbors=6, p=2..........................\n",
      "[CV 2/2; 30/162] END leaf_size=2, n_neighbors=6, p=2;, score=(train=0.705, test=0.569) total time=   0.2s\n",
      "[CV 1/2; 31/162] START leaf_size=2, n_neighbors=7, p=1..........................\n",
      "[CV 1/2; 31/162] END leaf_size=2, n_neighbors=7, p=1;, score=(train=0.667, test=0.587) total time=   0.6s\n",
      "[CV 2/2; 31/162] START leaf_size=2, n_neighbors=7, p=1..........................\n",
      "[CV 2/2; 31/162] END leaf_size=2, n_neighbors=7, p=1;, score=(train=0.698, test=0.578) total time=   0.6s\n",
      "[CV 1/2; 32/162] START leaf_size=2, n_neighbors=7, p=2..........................\n",
      "[CV 1/2; 32/162] END leaf_size=2, n_neighbors=7, p=2;, score=(train=0.659, test=0.570) total time=   0.2s\n",
      "[CV 2/2; 32/162] START leaf_size=2, n_neighbors=7, p=2..........................\n",
      "[CV 2/2; 32/162] END leaf_size=2, n_neighbors=7, p=2;, score=(train=0.685, test=0.561) total time=   0.2s\n",
      "[CV 1/2; 33/162] START leaf_size=2, n_neighbors=8, p=1..........................\n",
      "[CV 1/2; 33/162] END leaf_size=2, n_neighbors=8, p=1;, score=(train=0.665, test=0.586) total time=   0.6s\n",
      "[CV 2/2; 33/162] START leaf_size=2, n_neighbors=8, p=1..........................\n",
      "[CV 2/2; 33/162] END leaf_size=2, n_neighbors=8, p=1;, score=(train=0.683, test=0.578) total time=   0.6s\n",
      "[CV 1/2; 34/162] START leaf_size=2, n_neighbors=8, p=2..........................\n",
      "[CV 1/2; 34/162] END leaf_size=2, n_neighbors=8, p=2;, score=(train=0.643, test=0.574) total time=   0.2s\n",
      "[CV 2/2; 34/162] START leaf_size=2, n_neighbors=8, p=2..........................\n",
      "[CV 2/2; 34/162] END leaf_size=2, n_neighbors=8, p=2;, score=(train=0.673, test=0.566) total time=   0.1s\n",
      "[CV 1/2; 35/162] START leaf_size=2, n_neighbors=9, p=1..........................\n",
      "[CV 1/2; 35/162] END leaf_size=2, n_neighbors=9, p=1;, score=(train=0.651, test=0.583) total time=   0.5s\n",
      "[CV 2/2; 35/162] START leaf_size=2, n_neighbors=9, p=1..........................\n",
      "[CV 2/2; 35/162] END leaf_size=2, n_neighbors=9, p=1;, score=(train=0.675, test=0.575) total time=   0.6s\n",
      "[CV 1/2; 36/162] START leaf_size=2, n_neighbors=9, p=2..........................\n",
      "[CV 1/2; 36/162] END leaf_size=2, n_neighbors=9, p=2;, score=(train=0.632, test=0.568) total time=   0.2s\n",
      "[CV 2/2; 36/162] START leaf_size=2, n_neighbors=9, p=2..........................\n",
      "[CV 2/2; 36/162] END leaf_size=2, n_neighbors=9, p=2;, score=(train=0.652, test=0.565) total time=   0.2s\n",
      "[CV 1/2; 37/162] START leaf_size=3, n_neighbors=1, p=1..........................\n",
      "[CV 1/2; 37/162] END leaf_size=3, n_neighbors=1, p=1;, score=(train=1.000, test=0.575) total time=   0.5s\n",
      "[CV 2/2; 37/162] START leaf_size=3, n_neighbors=1, p=1..........................\n",
      "[CV 2/2; 37/162] END leaf_size=3, n_neighbors=1, p=1;, score=(train=1.000, test=0.571) total time=   0.5s\n",
      "[CV 1/2; 38/162] START leaf_size=3, n_neighbors=1, p=2..........................\n",
      "[CV 1/2; 38/162] END leaf_size=3, n_neighbors=1, p=2;, score=(train=1.000, test=0.560) total time=   0.2s\n",
      "[CV 2/2; 38/162] START leaf_size=3, n_neighbors=1, p=2..........................\n",
      "[CV 2/2; 38/162] END leaf_size=3, n_neighbors=1, p=2;, score=(train=1.000, test=0.552) total time=   0.1s\n",
      "[CV 1/2; 39/162] START leaf_size=3, n_neighbors=2, p=1..........................\n",
      "[CV 1/2; 39/162] END leaf_size=3, n_neighbors=2, p=1;, score=(train=0.827, test=0.539) total time=   0.6s\n",
      "[CV 2/2; 39/162] START leaf_size=3, n_neighbors=2, p=1..........................\n",
      "[CV 2/2; 39/162] END leaf_size=3, n_neighbors=2, p=1;, score=(train=0.840, test=0.539) total time=   0.6s\n",
      "[CV 1/2; 40/162] START leaf_size=3, n_neighbors=2, p=2..........................\n",
      "[CV 1/2; 40/162] END leaf_size=3, n_neighbors=2, p=2;, score=(train=0.825, test=0.531) total time=   0.2s\n",
      "[CV 2/2; 40/162] START leaf_size=3, n_neighbors=2, p=2..........................\n",
      "[CV 2/2; 40/162] END leaf_size=3, n_neighbors=2, p=2;, score=(train=0.840, test=0.504) total time=   0.2s\n",
      "[CV 1/2; 41/162] START leaf_size=3, n_neighbors=3, p=1..........................\n",
      "[CV 1/2; 41/162] END leaf_size=3, n_neighbors=3, p=1;, score=(train=0.762, test=0.574) total time=   0.6s\n",
      "[CV 2/2; 41/162] START leaf_size=3, n_neighbors=3, p=1..........................\n",
      "[CV 2/2; 41/162] END leaf_size=3, n_neighbors=3, p=1;, score=(train=0.784, test=0.569) total time=   0.6s\n",
      "[CV 1/2; 42/162] START leaf_size=3, n_neighbors=3, p=2..........................\n",
      "[CV 1/2; 42/162] END leaf_size=3, n_neighbors=3, p=2;, score=(train=0.756, test=0.560) total time=   0.2s\n",
      "[CV 2/2; 42/162] START leaf_size=3, n_neighbors=3, p=2..........................\n",
      "[CV 2/2; 42/162] END leaf_size=3, n_neighbors=3, p=2;, score=(train=0.783, test=0.555) total time=   0.2s\n",
      "[CV 1/2; 43/162] START leaf_size=3, n_neighbors=4, p=1..........................\n",
      "[CV 1/2; 43/162] END leaf_size=3, n_neighbors=4, p=1;, score=(train=0.736, test=0.593) total time=   0.6s\n",
      "[CV 2/2; 43/162] START leaf_size=3, n_neighbors=4, p=1..........................\n",
      "[CV 2/2; 43/162] END leaf_size=3, n_neighbors=4, p=1;, score=(train=0.753, test=0.578) total time=   0.6s\n",
      "[CV 1/2; 44/162] START leaf_size=3, n_neighbors=4, p=2..........................\n",
      "[CV 1/2; 44/162] END leaf_size=3, n_neighbors=4, p=2;, score=(train=0.723, test=0.578) total time=   0.2s\n",
      "[CV 2/2; 44/162] START leaf_size=3, n_neighbors=4, p=2..........................\n",
      "[CV 2/2; 44/162] END leaf_size=3, n_neighbors=4, p=2;, score=(train=0.740, test=0.566) total time=   0.2s\n",
      "[CV 1/2; 45/162] START leaf_size=3, n_neighbors=5, p=1..........................\n",
      "[CV 1/2; 45/162] END leaf_size=3, n_neighbors=5, p=1;, score=(train=0.709, test=0.585) total time=   0.6s\n",
      "[CV 2/2; 45/162] START leaf_size=3, n_neighbors=5, p=1..........................\n",
      "[CV 2/2; 45/162] END leaf_size=3, n_neighbors=5, p=1;, score=(train=0.730, test=0.575) total time=   0.9s\n",
      "[CV 1/2; 46/162] START leaf_size=3, n_neighbors=5, p=2..........................\n",
      "[CV 1/2; 46/162] END leaf_size=3, n_neighbors=5, p=2;, score=(train=0.698, test=0.575) total time=   0.3s\n",
      "[CV 2/2; 46/162] START leaf_size=3, n_neighbors=5, p=2..........................\n",
      "[CV 2/2; 46/162] END leaf_size=3, n_neighbors=5, p=2;, score=(train=0.713, test=0.565) total time=   0.2s\n",
      "[CV 1/2; 47/162] START leaf_size=3, n_neighbors=6, p=1..........................\n",
      "[CV 1/2; 47/162] END leaf_size=3, n_neighbors=6, p=1;, score=(train=0.689, test=0.595) total time=   0.7s\n",
      "[CV 2/2; 47/162] START leaf_size=3, n_neighbors=6, p=1..........................\n",
      "[CV 2/2; 47/162] END leaf_size=3, n_neighbors=6, p=1;, score=(train=0.705, test=0.569) total time=   0.6s\n",
      "[CV 1/2; 48/162] START leaf_size=3, n_neighbors=6, p=2..........................\n",
      "[CV 1/2; 48/162] END leaf_size=3, n_neighbors=6, p=2;, score=(train=0.680, test=0.568) total time=   0.3s\n",
      "[CV 2/2; 48/162] START leaf_size=3, n_neighbors=6, p=2..........................\n",
      "[CV 2/2; 48/162] END leaf_size=3, n_neighbors=6, p=2;, score=(train=0.705, test=0.569) total time=   0.2s\n",
      "[CV 1/2; 49/162] START leaf_size=3, n_neighbors=7, p=1..........................\n",
      "[CV 1/2; 49/162] END leaf_size=3, n_neighbors=7, p=1;, score=(train=0.667, test=0.587) total time=   0.6s\n",
      "[CV 2/2; 49/162] START leaf_size=3, n_neighbors=7, p=1..........................\n",
      "[CV 2/2; 49/162] END leaf_size=3, n_neighbors=7, p=1;, score=(train=0.698, test=0.578) total time=   0.6s\n",
      "[CV 1/2; 50/162] START leaf_size=3, n_neighbors=7, p=2..........................\n",
      "[CV 1/2; 50/162] END leaf_size=3, n_neighbors=7, p=2;, score=(train=0.659, test=0.570) total time=   0.3s\n",
      "[CV 2/2; 50/162] START leaf_size=3, n_neighbors=7, p=2..........................\n",
      "[CV 2/2; 50/162] END leaf_size=3, n_neighbors=7, p=2;, score=(train=0.685, test=0.561) total time=   0.1s\n",
      "[CV 1/2; 51/162] START leaf_size=3, n_neighbors=8, p=1..........................\n",
      "[CV 1/2; 51/162] END leaf_size=3, n_neighbors=8, p=1;, score=(train=0.665, test=0.586) total time=   0.6s\n",
      "[CV 2/2; 51/162] START leaf_size=3, n_neighbors=8, p=1..........................\n",
      "[CV 2/2; 51/162] END leaf_size=3, n_neighbors=8, p=1;, score=(train=0.683, test=0.578) total time=   0.6s\n",
      "[CV 1/2; 52/162] START leaf_size=3, n_neighbors=8, p=2..........................\n",
      "[CV 1/2; 52/162] END leaf_size=3, n_neighbors=8, p=2;, score=(train=0.643, test=0.574) total time=   0.2s\n",
      "[CV 2/2; 52/162] START leaf_size=3, n_neighbors=8, p=2..........................\n",
      "[CV 2/2; 52/162] END leaf_size=3, n_neighbors=8, p=2;, score=(train=0.673, test=0.566) total time=   0.2s\n",
      "[CV 1/2; 53/162] START leaf_size=3, n_neighbors=9, p=1..........................\n",
      "[CV 1/2; 53/162] END leaf_size=3, n_neighbors=9, p=1;, score=(train=0.651, test=0.583) total time=   0.5s\n",
      "[CV 2/2; 53/162] START leaf_size=3, n_neighbors=9, p=1..........................\n",
      "[CV 2/2; 53/162] END leaf_size=3, n_neighbors=9, p=1;, score=(train=0.675, test=0.575) total time=   0.6s\n",
      "[CV 1/2; 54/162] START leaf_size=3, n_neighbors=9, p=2..........................\n",
      "[CV 1/2; 54/162] END leaf_size=3, n_neighbors=9, p=2;, score=(train=0.632, test=0.568) total time=   0.2s\n",
      "[CV 2/2; 54/162] START leaf_size=3, n_neighbors=9, p=2..........................\n",
      "[CV 2/2; 54/162] END leaf_size=3, n_neighbors=9, p=2;, score=(train=0.652, test=0.565) total time=   0.2s\n",
      "[CV 1/2; 55/162] START leaf_size=4, n_neighbors=1, p=1..........................\n",
      "[CV 1/2; 55/162] END leaf_size=4, n_neighbors=1, p=1;, score=(train=1.000, test=0.575) total time=   0.6s\n",
      "[CV 2/2; 55/162] START leaf_size=4, n_neighbors=1, p=1..........................\n",
      "[CV 2/2; 55/162] END leaf_size=4, n_neighbors=1, p=1;, score=(train=1.000, test=0.571) total time=   0.5s\n",
      "[CV 1/2; 56/162] START leaf_size=4, n_neighbors=1, p=2..........................\n",
      "[CV 1/2; 56/162] END leaf_size=4, n_neighbors=1, p=2;, score=(train=1.000, test=0.560) total time=   0.2s\n",
      "[CV 2/2; 56/162] START leaf_size=4, n_neighbors=1, p=2..........................\n",
      "[CV 2/2; 56/162] END leaf_size=4, n_neighbors=1, p=2;, score=(train=1.000, test=0.552) total time=   0.2s\n",
      "[CV 1/2; 57/162] START leaf_size=4, n_neighbors=2, p=1..........................\n",
      "[CV 1/2; 57/162] END leaf_size=4, n_neighbors=2, p=1;, score=(train=0.827, test=0.539) total time=   0.6s\n",
      "[CV 2/2; 57/162] START leaf_size=4, n_neighbors=2, p=1..........................\n",
      "[CV 2/2; 57/162] END leaf_size=4, n_neighbors=2, p=1;, score=(train=0.840, test=0.539) total time=   0.5s\n",
      "[CV 1/2; 58/162] START leaf_size=4, n_neighbors=2, p=2..........................\n",
      "[CV 1/2; 58/162] END leaf_size=4, n_neighbors=2, p=2;, score=(train=0.825, test=0.531) total time=   0.2s\n",
      "[CV 2/2; 58/162] START leaf_size=4, n_neighbors=2, p=2..........................\n",
      "[CV 2/2; 58/162] END leaf_size=4, n_neighbors=2, p=2;, score=(train=0.840, test=0.504) total time=   0.2s\n",
      "[CV 1/2; 59/162] START leaf_size=4, n_neighbors=3, p=1..........................\n",
      "[CV 1/2; 59/162] END leaf_size=4, n_neighbors=3, p=1;, score=(train=0.762, test=0.574) total time=   0.6s\n",
      "[CV 2/2; 59/162] START leaf_size=4, n_neighbors=3, p=1..........................\n",
      "[CV 2/2; 59/162] END leaf_size=4, n_neighbors=3, p=1;, score=(train=0.784, test=0.569) total time=   0.5s\n",
      "[CV 1/2; 60/162] START leaf_size=4, n_neighbors=3, p=2..........................\n",
      "[CV 1/2; 60/162] END leaf_size=4, n_neighbors=3, p=2;, score=(train=0.756, test=0.560) total time=   0.2s\n",
      "[CV 2/2; 60/162] START leaf_size=4, n_neighbors=3, p=2..........................\n",
      "[CV 2/2; 60/162] END leaf_size=4, n_neighbors=3, p=2;, score=(train=0.783, test=0.555) total time=   0.2s\n",
      "[CV 1/2; 61/162] START leaf_size=4, n_neighbors=4, p=1..........................\n",
      "[CV 1/2; 61/162] END leaf_size=4, n_neighbors=4, p=1;, score=(train=0.736, test=0.593) total time=   0.6s\n",
      "[CV 2/2; 61/162] START leaf_size=4, n_neighbors=4, p=1..........................\n",
      "[CV 2/2; 61/162] END leaf_size=4, n_neighbors=4, p=1;, score=(train=0.753, test=0.578) total time=   0.6s\n",
      "[CV 1/2; 62/162] START leaf_size=4, n_neighbors=4, p=2..........................\n",
      "[CV 1/2; 62/162] END leaf_size=4, n_neighbors=4, p=2;, score=(train=0.723, test=0.578) total time=   0.2s\n",
      "[CV 2/2; 62/162] START leaf_size=4, n_neighbors=4, p=2..........................\n",
      "[CV 2/2; 62/162] END leaf_size=4, n_neighbors=4, p=2;, score=(train=0.740, test=0.566) total time=   0.2s\n",
      "[CV 1/2; 63/162] START leaf_size=4, n_neighbors=5, p=1..........................\n",
      "[CV 1/2; 63/162] END leaf_size=4, n_neighbors=5, p=1;, score=(train=0.709, test=0.585) total time=   0.5s\n",
      "[CV 2/2; 63/162] START leaf_size=4, n_neighbors=5, p=1..........................\n",
      "[CV 2/2; 63/162] END leaf_size=4, n_neighbors=5, p=1;, score=(train=0.730, test=0.575) total time=   0.7s\n",
      "[CV 1/2; 64/162] START leaf_size=4, n_neighbors=5, p=2..........................\n",
      "[CV 1/2; 64/162] END leaf_size=4, n_neighbors=5, p=2;, score=(train=0.698, test=0.575) total time=   0.3s\n",
      "[CV 2/2; 64/162] START leaf_size=4, n_neighbors=5, p=2..........................\n",
      "[CV 2/2; 64/162] END leaf_size=4, n_neighbors=5, p=2;, score=(train=0.713, test=0.565) total time=   0.3s\n",
      "[CV 1/2; 65/162] START leaf_size=4, n_neighbors=6, p=1..........................\n",
      "[CV 1/2; 65/162] END leaf_size=4, n_neighbors=6, p=1;, score=(train=0.689, test=0.595) total time=   0.6s\n",
      "[CV 2/2; 65/162] START leaf_size=4, n_neighbors=6, p=1..........................\n",
      "[CV 2/2; 65/162] END leaf_size=4, n_neighbors=6, p=1;, score=(train=0.705, test=0.569) total time=   0.6s\n",
      "[CV 1/2; 66/162] START leaf_size=4, n_neighbors=6, p=2..........................\n",
      "[CV 1/2; 66/162] END leaf_size=4, n_neighbors=6, p=2;, score=(train=0.680, test=0.568) total time=   0.2s\n",
      "[CV 2/2; 66/162] START leaf_size=4, n_neighbors=6, p=2..........................\n",
      "[CV 2/2; 66/162] END leaf_size=4, n_neighbors=6, p=2;, score=(train=0.705, test=0.569) total time=   0.2s\n",
      "[CV 1/2; 67/162] START leaf_size=4, n_neighbors=7, p=1..........................\n",
      "[CV 1/2; 67/162] END leaf_size=4, n_neighbors=7, p=1;, score=(train=0.667, test=0.587) total time=   0.6s\n",
      "[CV 2/2; 67/162] START leaf_size=4, n_neighbors=7, p=1..........................\n",
      "[CV 2/2; 67/162] END leaf_size=4, n_neighbors=7, p=1;, score=(train=0.698, test=0.578) total time=   0.6s\n",
      "[CV 1/2; 68/162] START leaf_size=4, n_neighbors=7, p=2..........................\n",
      "[CV 1/2; 68/162] END leaf_size=4, n_neighbors=7, p=2;, score=(train=0.659, test=0.570) total time=   0.2s\n",
      "[CV 2/2; 68/162] START leaf_size=4, n_neighbors=7, p=2..........................\n",
      "[CV 2/2; 68/162] END leaf_size=4, n_neighbors=7, p=2;, score=(train=0.685, test=0.561) total time=   0.2s\n",
      "[CV 1/2; 69/162] START leaf_size=4, n_neighbors=8, p=1..........................\n",
      "[CV 1/2; 69/162] END leaf_size=4, n_neighbors=8, p=1;, score=(train=0.665, test=0.586) total time=   0.6s\n",
      "[CV 2/2; 69/162] START leaf_size=4, n_neighbors=8, p=1..........................\n",
      "[CV 2/2; 69/162] END leaf_size=4, n_neighbors=8, p=1;, score=(train=0.683, test=0.578) total time=   0.5s\n",
      "[CV 1/2; 70/162] START leaf_size=4, n_neighbors=8, p=2..........................\n",
      "[CV 1/2; 70/162] END leaf_size=4, n_neighbors=8, p=2;, score=(train=0.643, test=0.574) total time=   0.2s\n",
      "[CV 2/2; 70/162] START leaf_size=4, n_neighbors=8, p=2..........................\n",
      "[CV 2/2; 70/162] END leaf_size=4, n_neighbors=8, p=2;, score=(train=0.673, test=0.566) total time=   0.2s\n",
      "[CV 1/2; 71/162] START leaf_size=4, n_neighbors=9, p=1..........................\n",
      "[CV 1/2; 71/162] END leaf_size=4, n_neighbors=9, p=1;, score=(train=0.651, test=0.583) total time=   0.9s\n",
      "[CV 2/2; 71/162] START leaf_size=4, n_neighbors=9, p=1..........................\n",
      "[CV 2/2; 71/162] END leaf_size=4, n_neighbors=9, p=1;, score=(train=0.675, test=0.575) total time=   0.6s\n",
      "[CV 1/2; 72/162] START leaf_size=4, n_neighbors=9, p=2..........................\n",
      "[CV 1/2; 72/162] END leaf_size=4, n_neighbors=9, p=2;, score=(train=0.632, test=0.568) total time=   0.2s\n",
      "[CV 2/2; 72/162] START leaf_size=4, n_neighbors=9, p=2..........................\n",
      "[CV 2/2; 72/162] END leaf_size=4, n_neighbors=9, p=2;, score=(train=0.652, test=0.565) total time=   0.2s\n",
      "[CV 1/2; 73/162] START leaf_size=5, n_neighbors=1, p=1..........................\n",
      "[CV 1/2; 73/162] END leaf_size=5, n_neighbors=1, p=1;, score=(train=1.000, test=0.575) total time=   0.5s\n",
      "[CV 2/2; 73/162] START leaf_size=5, n_neighbors=1, p=1..........................\n",
      "[CV 2/2; 73/162] END leaf_size=5, n_neighbors=1, p=1;, score=(train=1.000, test=0.571) total time=   0.5s\n",
      "[CV 1/2; 74/162] START leaf_size=5, n_neighbors=1, p=2..........................\n",
      "[CV 1/2; 74/162] END leaf_size=5, n_neighbors=1, p=2;, score=(train=1.000, test=0.560) total time=   0.2s\n",
      "[CV 2/2; 74/162] START leaf_size=5, n_neighbors=1, p=2..........................\n",
      "[CV 2/2; 74/162] END leaf_size=5, n_neighbors=1, p=2;, score=(train=1.000, test=0.552) total time=   0.1s\n",
      "[CV 1/2; 75/162] START leaf_size=5, n_neighbors=2, p=1..........................\n",
      "[CV 1/2; 75/162] END leaf_size=5, n_neighbors=2, p=1;, score=(train=0.827, test=0.539) total time=   0.6s\n",
      "[CV 2/2; 75/162] START leaf_size=5, n_neighbors=2, p=1..........................\n",
      "[CV 2/2; 75/162] END leaf_size=5, n_neighbors=2, p=1;, score=(train=0.840, test=0.539) total time=   0.6s\n",
      "[CV 1/2; 76/162] START leaf_size=5, n_neighbors=2, p=2..........................\n",
      "[CV 1/2; 76/162] END leaf_size=5, n_neighbors=2, p=2;, score=(train=0.825, test=0.531) total time=   0.2s\n",
      "[CV 2/2; 76/162] START leaf_size=5, n_neighbors=2, p=2..........................\n",
      "[CV 2/2; 76/162] END leaf_size=5, n_neighbors=2, p=2;, score=(train=0.840, test=0.504) total time=   0.2s\n",
      "[CV 1/2; 77/162] START leaf_size=5, n_neighbors=3, p=1..........................\n",
      "[CV 1/2; 77/162] END leaf_size=5, n_neighbors=3, p=1;, score=(train=0.762, test=0.574) total time=   0.5s\n",
      "[CV 2/2; 77/162] START leaf_size=5, n_neighbors=3, p=1..........................\n",
      "[CV 2/2; 77/162] END leaf_size=5, n_neighbors=3, p=1;, score=(train=0.784, test=0.569) total time=   0.6s\n",
      "[CV 1/2; 78/162] START leaf_size=5, n_neighbors=3, p=2..........................\n",
      "[CV 1/2; 78/162] END leaf_size=5, n_neighbors=3, p=2;, score=(train=0.756, test=0.560) total time=   0.1s\n",
      "[CV 2/2; 78/162] START leaf_size=5, n_neighbors=3, p=2..........................\n",
      "[CV 2/2; 78/162] END leaf_size=5, n_neighbors=3, p=2;, score=(train=0.783, test=0.555) total time=   0.2s\n",
      "[CV 1/2; 79/162] START leaf_size=5, n_neighbors=4, p=1..........................\n",
      "[CV 1/2; 79/162] END leaf_size=5, n_neighbors=4, p=1;, score=(train=0.736, test=0.593) total time=   0.5s\n",
      "[CV 2/2; 79/162] START leaf_size=5, n_neighbors=4, p=1..........................\n",
      "[CV 2/2; 79/162] END leaf_size=5, n_neighbors=4, p=1;, score=(train=0.753, test=0.578) total time=   0.6s\n",
      "[CV 1/2; 80/162] START leaf_size=5, n_neighbors=4, p=2..........................\n",
      "[CV 1/2; 80/162] END leaf_size=5, n_neighbors=4, p=2;, score=(train=0.723, test=0.578) total time=   0.2s\n",
      "[CV 2/2; 80/162] START leaf_size=5, n_neighbors=4, p=2..........................\n",
      "[CV 2/2; 80/162] END leaf_size=5, n_neighbors=4, p=2;, score=(train=0.740, test=0.566) total time=   0.2s\n",
      "[CV 1/2; 81/162] START leaf_size=5, n_neighbors=5, p=1..........................\n",
      "[CV 1/2; 81/162] END leaf_size=5, n_neighbors=5, p=1;, score=(train=0.709, test=0.585) total time=   0.5s\n",
      "[CV 2/2; 81/162] START leaf_size=5, n_neighbors=5, p=1..........................\n",
      "[CV 2/2; 81/162] END leaf_size=5, n_neighbors=5, p=1;, score=(train=0.730, test=0.575) total time=   0.5s\n",
      "[CV 1/2; 82/162] START leaf_size=5, n_neighbors=5, p=2..........................\n",
      "[CV 1/2; 82/162] END leaf_size=5, n_neighbors=5, p=2;, score=(train=0.698, test=0.575) total time=   0.2s\n",
      "[CV 2/2; 82/162] START leaf_size=5, n_neighbors=5, p=2..........................\n",
      "[CV 2/2; 82/162] END leaf_size=5, n_neighbors=5, p=2;, score=(train=0.713, test=0.565) total time=   0.2s\n",
      "[CV 1/2; 83/162] START leaf_size=5, n_neighbors=6, p=1..........................\n",
      "[CV 1/2; 83/162] END leaf_size=5, n_neighbors=6, p=1;, score=(train=0.689, test=0.595) total time=   0.6s\n",
      "[CV 2/2; 83/162] START leaf_size=5, n_neighbors=6, p=1..........................\n",
      "[CV 2/2; 83/162] END leaf_size=5, n_neighbors=6, p=1;, score=(train=0.705, test=0.569) total time=   0.6s\n",
      "[CV 1/2; 84/162] START leaf_size=5, n_neighbors=6, p=2..........................\n",
      "[CV 1/2; 84/162] END leaf_size=5, n_neighbors=6, p=2;, score=(train=0.680, test=0.568) total time=   0.2s\n",
      "[CV 2/2; 84/162] START leaf_size=5, n_neighbors=6, p=2..........................\n",
      "[CV 2/2; 84/162] END leaf_size=5, n_neighbors=6, p=2;, score=(train=0.705, test=0.569) total time=   0.2s\n",
      "[CV 1/2; 85/162] START leaf_size=5, n_neighbors=7, p=1..........................\n",
      "[CV 1/2; 85/162] END leaf_size=5, n_neighbors=7, p=1;, score=(train=0.667, test=0.587) total time=   0.6s\n",
      "[CV 2/2; 85/162] START leaf_size=5, n_neighbors=7, p=1..........................\n",
      "[CV 2/2; 85/162] END leaf_size=5, n_neighbors=7, p=1;, score=(train=0.698, test=0.578) total time=   0.6s\n",
      "[CV 1/2; 86/162] START leaf_size=5, n_neighbors=7, p=2..........................\n",
      "[CV 1/2; 86/162] END leaf_size=5, n_neighbors=7, p=2;, score=(train=0.659, test=0.570) total time=   0.1s\n",
      "[CV 2/2; 86/162] START leaf_size=5, n_neighbors=7, p=2..........................\n",
      "[CV 2/2; 86/162] END leaf_size=5, n_neighbors=7, p=2;, score=(train=0.685, test=0.561) total time=   0.2s\n",
      "[CV 1/2; 87/162] START leaf_size=5, n_neighbors=8, p=1..........................\n",
      "[CV 1/2; 87/162] END leaf_size=5, n_neighbors=8, p=1;, score=(train=0.665, test=0.586) total time=   0.6s\n",
      "[CV 2/2; 87/162] START leaf_size=5, n_neighbors=8, p=1..........................\n",
      "[CV 2/2; 87/162] END leaf_size=5, n_neighbors=8, p=1;, score=(train=0.683, test=0.578) total time=   0.6s\n",
      "[CV 1/2; 88/162] START leaf_size=5, n_neighbors=8, p=2..........................\n",
      "[CV 1/2; 88/162] END leaf_size=5, n_neighbors=8, p=2;, score=(train=0.643, test=0.574) total time=   0.2s\n",
      "[CV 2/2; 88/162] START leaf_size=5, n_neighbors=8, p=2..........................\n",
      "[CV 2/2; 88/162] END leaf_size=5, n_neighbors=8, p=2;, score=(train=0.673, test=0.566) total time=   0.2s\n",
      "[CV 1/2; 89/162] START leaf_size=5, n_neighbors=9, p=1..........................\n",
      "[CV 1/2; 89/162] END leaf_size=5, n_neighbors=9, p=1;, score=(train=0.651, test=0.583) total time=   0.6s\n",
      "[CV 2/2; 89/162] START leaf_size=5, n_neighbors=9, p=1..........................\n",
      "[CV 2/2; 89/162] END leaf_size=5, n_neighbors=9, p=1;, score=(train=0.675, test=0.575) total time=   0.5s\n",
      "[CV 1/2; 90/162] START leaf_size=5, n_neighbors=9, p=2..........................\n",
      "[CV 1/2; 90/162] END leaf_size=5, n_neighbors=9, p=2;, score=(train=0.632, test=0.568) total time=   0.3s\n",
      "[CV 2/2; 90/162] START leaf_size=5, n_neighbors=9, p=2..........................\n",
      "[CV 2/2; 90/162] END leaf_size=5, n_neighbors=9, p=2;, score=(train=0.652, test=0.565) total time=   0.2s\n",
      "[CV 1/2; 91/162] START leaf_size=6, n_neighbors=1, p=1..........................\n",
      "[CV 1/2; 91/162] END leaf_size=6, n_neighbors=1, p=1;, score=(train=1.000, test=0.575) total time=   0.5s\n",
      "[CV 2/2; 91/162] START leaf_size=6, n_neighbors=1, p=1..........................\n",
      "[CV 2/2; 91/162] END leaf_size=6, n_neighbors=1, p=1;, score=(train=1.000, test=0.571) total time=   0.5s\n",
      "[CV 1/2; 92/162] START leaf_size=6, n_neighbors=1, p=2..........................\n",
      "[CV 1/2; 92/162] END leaf_size=6, n_neighbors=1, p=2;, score=(train=1.000, test=0.560) total time=   0.2s\n",
      "[CV 2/2; 92/162] START leaf_size=6, n_neighbors=1, p=2..........................\n",
      "[CV 2/2; 92/162] END leaf_size=6, n_neighbors=1, p=2;, score=(train=1.000, test=0.552) total time=   0.1s\n",
      "[CV 1/2; 93/162] START leaf_size=6, n_neighbors=2, p=1..........................\n",
      "[CV 1/2; 93/162] END leaf_size=6, n_neighbors=2, p=1;, score=(train=0.827, test=0.539) total time=   0.6s\n",
      "[CV 2/2; 93/162] START leaf_size=6, n_neighbors=2, p=1..........................\n",
      "[CV 2/2; 93/162] END leaf_size=6, n_neighbors=2, p=1;, score=(train=0.840, test=0.539) total time=   0.5s\n",
      "[CV 1/2; 94/162] START leaf_size=6, n_neighbors=2, p=2..........................\n",
      "[CV 1/2; 94/162] END leaf_size=6, n_neighbors=2, p=2;, score=(train=0.825, test=0.531) total time=   0.2s\n",
      "[CV 2/2; 94/162] START leaf_size=6, n_neighbors=2, p=2..........................\n",
      "[CV 2/2; 94/162] END leaf_size=6, n_neighbors=2, p=2;, score=(train=0.840, test=0.504) total time=   0.2s\n",
      "[CV 1/2; 95/162] START leaf_size=6, n_neighbors=3, p=1..........................\n",
      "[CV 1/2; 95/162] END leaf_size=6, n_neighbors=3, p=1;, score=(train=0.762, test=0.574) total time=   0.5s\n",
      "[CV 2/2; 95/162] START leaf_size=6, n_neighbors=3, p=1..........................\n",
      "[CV 2/2; 95/162] END leaf_size=6, n_neighbors=3, p=1;, score=(train=0.784, test=0.569) total time=   0.6s\n",
      "[CV 1/2; 96/162] START leaf_size=6, n_neighbors=3, p=2..........................\n",
      "[CV 1/2; 96/162] END leaf_size=6, n_neighbors=3, p=2;, score=(train=0.756, test=0.560) total time=   0.2s\n",
      "[CV 2/2; 96/162] START leaf_size=6, n_neighbors=3, p=2..........................\n",
      "[CV 2/2; 96/162] END leaf_size=6, n_neighbors=3, p=2;, score=(train=0.783, test=0.555) total time=   0.2s\n",
      "[CV 1/2; 97/162] START leaf_size=6, n_neighbors=4, p=1..........................\n",
      "[CV 1/2; 97/162] END leaf_size=6, n_neighbors=4, p=1;, score=(train=0.736, test=0.593) total time=   0.5s\n",
      "[CV 2/2; 97/162] START leaf_size=6, n_neighbors=4, p=1..........................\n",
      "[CV 2/2; 97/162] END leaf_size=6, n_neighbors=4, p=1;, score=(train=0.753, test=0.578) total time=   0.5s\n",
      "[CV 1/2; 98/162] START leaf_size=6, n_neighbors=4, p=2..........................\n",
      "[CV 1/2; 98/162] END leaf_size=6, n_neighbors=4, p=2;, score=(train=0.723, test=0.578) total time=   0.2s\n",
      "[CV 2/2; 98/162] START leaf_size=6, n_neighbors=4, p=2..........................\n",
      "[CV 2/2; 98/162] END leaf_size=6, n_neighbors=4, p=2;, score=(train=0.740, test=0.566) total time=   0.2s\n",
      "[CV 1/2; 99/162] START leaf_size=6, n_neighbors=5, p=1..........................\n",
      "[CV 1/2; 99/162] END leaf_size=6, n_neighbors=5, p=1;, score=(train=0.709, test=0.585) total time=   0.5s\n",
      "[CV 2/2; 99/162] START leaf_size=6, n_neighbors=5, p=1..........................\n",
      "[CV 2/2; 99/162] END leaf_size=6, n_neighbors=5, p=1;, score=(train=0.730, test=0.575) total time=   0.6s\n",
      "[CV 1/2; 100/162] START leaf_size=6, n_neighbors=5, p=2.........................\n",
      "[CV 1/2; 100/162] END leaf_size=6, n_neighbors=5, p=2;, score=(train=0.698, test=0.575) total time=   0.2s\n",
      "[CV 2/2; 100/162] START leaf_size=6, n_neighbors=5, p=2.........................\n",
      "[CV 2/2; 100/162] END leaf_size=6, n_neighbors=5, p=2;, score=(train=0.713, test=0.565) total time=   0.2s\n",
      "[CV 1/2; 101/162] START leaf_size=6, n_neighbors=6, p=1.........................\n",
      "[CV 1/2; 101/162] END leaf_size=6, n_neighbors=6, p=1;, score=(train=0.689, test=0.595) total time=   0.5s\n",
      "[CV 2/2; 101/162] START leaf_size=6, n_neighbors=6, p=1.........................\n",
      "[CV 2/2; 101/162] END leaf_size=6, n_neighbors=6, p=1;, score=(train=0.705, test=0.569) total time=   0.6s\n",
      "[CV 1/2; 102/162] START leaf_size=6, n_neighbors=6, p=2.........................\n",
      "[CV 1/2; 102/162] END leaf_size=6, n_neighbors=6, p=2;, score=(train=0.680, test=0.568) total time=   0.2s\n",
      "[CV 2/2; 102/162] START leaf_size=6, n_neighbors=6, p=2.........................\n",
      "[CV 2/2; 102/162] END leaf_size=6, n_neighbors=6, p=2;, score=(train=0.705, test=0.569) total time=   0.2s\n",
      "[CV 1/2; 103/162] START leaf_size=6, n_neighbors=7, p=1.........................\n",
      "[CV 1/2; 103/162] END leaf_size=6, n_neighbors=7, p=1;, score=(train=0.667, test=0.587) total time=   0.6s\n",
      "[CV 2/2; 103/162] START leaf_size=6, n_neighbors=7, p=1.........................\n",
      "[CV 2/2; 103/162] END leaf_size=6, n_neighbors=7, p=1;, score=(train=0.698, test=0.578) total time=   0.6s\n",
      "[CV 1/2; 104/162] START leaf_size=6, n_neighbors=7, p=2.........................\n",
      "[CV 1/2; 104/162] END leaf_size=6, n_neighbors=7, p=2;, score=(train=0.659, test=0.570) total time=   0.2s\n",
      "[CV 2/2; 104/162] START leaf_size=6, n_neighbors=7, p=2.........................\n",
      "[CV 2/2; 104/162] END leaf_size=6, n_neighbors=7, p=2;, score=(train=0.685, test=0.561) total time=   0.2s\n",
      "[CV 1/2; 105/162] START leaf_size=6, n_neighbors=8, p=1.........................\n",
      "[CV 1/2; 105/162] END leaf_size=6, n_neighbors=8, p=1;, score=(train=0.665, test=0.586) total time=   0.6s\n",
      "[CV 2/2; 105/162] START leaf_size=6, n_neighbors=8, p=1.........................\n",
      "[CV 2/2; 105/162] END leaf_size=6, n_neighbors=8, p=1;, score=(train=0.683, test=0.578) total time=   0.5s\n",
      "[CV 1/2; 106/162] START leaf_size=6, n_neighbors=8, p=2.........................\n",
      "[CV 1/2; 106/162] END leaf_size=6, n_neighbors=8, p=2;, score=(train=0.643, test=0.574) total time=   0.2s\n",
      "[CV 2/2; 106/162] START leaf_size=6, n_neighbors=8, p=2.........................\n",
      "[CV 2/2; 106/162] END leaf_size=6, n_neighbors=8, p=2;, score=(train=0.673, test=0.566) total time=   0.2s\n",
      "[CV 1/2; 107/162] START leaf_size=6, n_neighbors=9, p=1.........................\n",
      "[CV 1/2; 107/162] END leaf_size=6, n_neighbors=9, p=1;, score=(train=0.651, test=0.583) total time=   0.6s\n",
      "[CV 2/2; 107/162] START leaf_size=6, n_neighbors=9, p=1.........................\n",
      "[CV 2/2; 107/162] END leaf_size=6, n_neighbors=9, p=1;, score=(train=0.675, test=0.575) total time=   0.6s\n",
      "[CV 1/2; 108/162] START leaf_size=6, n_neighbors=9, p=2.........................\n",
      "[CV 1/2; 108/162] END leaf_size=6, n_neighbors=9, p=2;, score=(train=0.632, test=0.568) total time=   0.3s\n",
      "[CV 2/2; 108/162] START leaf_size=6, n_neighbors=9, p=2.........................\n",
      "[CV 2/2; 108/162] END leaf_size=6, n_neighbors=9, p=2;, score=(train=0.652, test=0.565) total time=   0.2s\n",
      "[CV 1/2; 109/162] START leaf_size=7, n_neighbors=1, p=1.........................\n",
      "[CV 1/2; 109/162] END leaf_size=7, n_neighbors=1, p=1;, score=(train=1.000, test=0.575) total time=   0.6s\n",
      "[CV 2/2; 109/162] START leaf_size=7, n_neighbors=1, p=1.........................\n",
      "[CV 2/2; 109/162] END leaf_size=7, n_neighbors=1, p=1;, score=(train=1.000, test=0.571) total time=   0.5s\n",
      "[CV 1/2; 110/162] START leaf_size=7, n_neighbors=1, p=2.........................\n",
      "[CV 1/2; 110/162] END leaf_size=7, n_neighbors=1, p=2;, score=(train=1.000, test=0.560) total time=   0.2s\n",
      "[CV 2/2; 110/162] START leaf_size=7, n_neighbors=1, p=2.........................\n",
      "[CV 2/2; 110/162] END leaf_size=7, n_neighbors=1, p=2;, score=(train=1.000, test=0.552) total time=   0.2s\n",
      "[CV 1/2; 111/162] START leaf_size=7, n_neighbors=2, p=1.........................\n",
      "[CV 1/2; 111/162] END leaf_size=7, n_neighbors=2, p=1;, score=(train=0.827, test=0.539) total time=   0.5s\n",
      "[CV 2/2; 111/162] START leaf_size=7, n_neighbors=2, p=1.........................\n",
      "[CV 2/2; 111/162] END leaf_size=7, n_neighbors=2, p=1;, score=(train=0.840, test=0.539) total time=   0.6s\n",
      "[CV 1/2; 112/162] START leaf_size=7, n_neighbors=2, p=2.........................\n",
      "[CV 1/2; 112/162] END leaf_size=7, n_neighbors=2, p=2;, score=(train=0.825, test=0.531) total time=   0.2s\n",
      "[CV 2/2; 112/162] START leaf_size=7, n_neighbors=2, p=2.........................\n",
      "[CV 2/2; 112/162] END leaf_size=7, n_neighbors=2, p=2;, score=(train=0.840, test=0.504) total time=   0.2s\n",
      "[CV 1/2; 113/162] START leaf_size=7, n_neighbors=3, p=1.........................\n",
      "[CV 1/2; 113/162] END leaf_size=7, n_neighbors=3, p=1;, score=(train=0.762, test=0.574) total time=   0.5s\n",
      "[CV 2/2; 113/162] START leaf_size=7, n_neighbors=3, p=1.........................\n",
      "[CV 2/2; 113/162] END leaf_size=7, n_neighbors=3, p=1;, score=(train=0.784, test=0.569) total time=   0.6s\n",
      "[CV 1/2; 114/162] START leaf_size=7, n_neighbors=3, p=2.........................\n",
      "[CV 1/2; 114/162] END leaf_size=7, n_neighbors=3, p=2;, score=(train=0.756, test=0.560) total time=   0.2s\n",
      "[CV 2/2; 114/162] START leaf_size=7, n_neighbors=3, p=2.........................\n",
      "[CV 2/2; 114/162] END leaf_size=7, n_neighbors=3, p=2;, score=(train=0.783, test=0.555) total time=   0.2s\n",
      "[CV 1/2; 115/162] START leaf_size=7, n_neighbors=4, p=1.........................\n",
      "[CV 1/2; 115/162] END leaf_size=7, n_neighbors=4, p=1;, score=(train=0.736, test=0.593) total time=   0.6s\n",
      "[CV 2/2; 115/162] START leaf_size=7, n_neighbors=4, p=1.........................\n",
      "[CV 2/2; 115/162] END leaf_size=7, n_neighbors=4, p=1;, score=(train=0.753, test=0.578) total time=   0.6s\n",
      "[CV 1/2; 116/162] START leaf_size=7, n_neighbors=4, p=2.........................\n",
      "[CV 1/2; 116/162] END leaf_size=7, n_neighbors=4, p=2;, score=(train=0.723, test=0.578) total time=   0.2s\n",
      "[CV 2/2; 116/162] START leaf_size=7, n_neighbors=4, p=2.........................\n",
      "[CV 2/2; 116/162] END leaf_size=7, n_neighbors=4, p=2;, score=(train=0.740, test=0.566) total time=   0.1s\n",
      "[CV 1/2; 117/162] START leaf_size=7, n_neighbors=5, p=1.........................\n",
      "[CV 1/2; 117/162] END leaf_size=7, n_neighbors=5, p=1;, score=(train=0.709, test=0.585) total time=   0.6s\n",
      "[CV 2/2; 117/162] START leaf_size=7, n_neighbors=5, p=1.........................\n",
      "[CV 2/2; 117/162] END leaf_size=7, n_neighbors=5, p=1;, score=(train=0.730, test=0.575) total time=   0.6s\n",
      "[CV 1/2; 118/162] START leaf_size=7, n_neighbors=5, p=2.........................\n",
      "[CV 1/2; 118/162] END leaf_size=7, n_neighbors=5, p=2;, score=(train=0.698, test=0.575) total time=   0.2s\n",
      "[CV 2/2; 118/162] START leaf_size=7, n_neighbors=5, p=2.........................\n",
      "[CV 2/2; 118/162] END leaf_size=7, n_neighbors=5, p=2;, score=(train=0.713, test=0.565) total time=   0.2s\n",
      "[CV 1/2; 119/162] START leaf_size=7, n_neighbors=6, p=1.........................\n",
      "[CV 1/2; 119/162] END leaf_size=7, n_neighbors=6, p=1;, score=(train=0.689, test=0.595) total time=   0.6s\n",
      "[CV 2/2; 119/162] START leaf_size=7, n_neighbors=6, p=1.........................\n",
      "[CV 2/2; 119/162] END leaf_size=7, n_neighbors=6, p=1;, score=(train=0.705, test=0.569) total time=   0.6s\n",
      "[CV 1/2; 120/162] START leaf_size=7, n_neighbors=6, p=2.........................\n",
      "[CV 1/2; 120/162] END leaf_size=7, n_neighbors=6, p=2;, score=(train=0.680, test=0.568) total time=   0.2s\n",
      "[CV 2/2; 120/162] START leaf_size=7, n_neighbors=6, p=2.........................\n",
      "[CV 2/2; 120/162] END leaf_size=7, n_neighbors=6, p=2;, score=(train=0.705, test=0.569) total time=   0.2s\n",
      "[CV 1/2; 121/162] START leaf_size=7, n_neighbors=7, p=1.........................\n",
      "[CV 1/2; 121/162] END leaf_size=7, n_neighbors=7, p=1;, score=(train=0.667, test=0.587) total time=   0.6s\n",
      "[CV 2/2; 121/162] START leaf_size=7, n_neighbors=7, p=1.........................\n",
      "[CV 2/2; 121/162] END leaf_size=7, n_neighbors=7, p=1;, score=(train=0.698, test=0.578) total time=   0.6s\n",
      "[CV 1/2; 122/162] START leaf_size=7, n_neighbors=7, p=2.........................\n",
      "[CV 1/2; 122/162] END leaf_size=7, n_neighbors=7, p=2;, score=(train=0.659, test=0.570) total time=   0.2s\n",
      "[CV 2/2; 122/162] START leaf_size=7, n_neighbors=7, p=2.........................\n",
      "[CV 2/2; 122/162] END leaf_size=7, n_neighbors=7, p=2;, score=(train=0.685, test=0.561) total time=   0.2s\n",
      "[CV 1/2; 123/162] START leaf_size=7, n_neighbors=8, p=1.........................\n",
      "[CV 1/2; 123/162] END leaf_size=7, n_neighbors=8, p=1;, score=(train=0.665, test=0.586) total time=   0.6s\n",
      "[CV 2/2; 123/162] START leaf_size=7, n_neighbors=8, p=1.........................\n",
      "[CV 2/2; 123/162] END leaf_size=7, n_neighbors=8, p=1;, score=(train=0.683, test=0.578) total time=   0.6s\n",
      "[CV 1/2; 124/162] START leaf_size=7, n_neighbors=8, p=2.........................\n",
      "[CV 1/2; 124/162] END leaf_size=7, n_neighbors=8, p=2;, score=(train=0.643, test=0.574) total time=   0.2s\n",
      "[CV 2/2; 124/162] START leaf_size=7, n_neighbors=8, p=2.........................\n",
      "[CV 2/2; 124/162] END leaf_size=7, n_neighbors=8, p=2;, score=(train=0.673, test=0.566) total time=   0.2s\n",
      "[CV 1/2; 125/162] START leaf_size=7, n_neighbors=9, p=1.........................\n",
      "[CV 1/2; 125/162] END leaf_size=7, n_neighbors=9, p=1;, score=(train=0.651, test=0.583) total time=   0.6s\n",
      "[CV 2/2; 125/162] START leaf_size=7, n_neighbors=9, p=1.........................\n",
      "[CV 2/2; 125/162] END leaf_size=7, n_neighbors=9, p=1;, score=(train=0.675, test=0.575) total time=   0.6s\n",
      "[CV 1/2; 126/162] START leaf_size=7, n_neighbors=9, p=2.........................\n",
      "[CV 1/2; 126/162] END leaf_size=7, n_neighbors=9, p=2;, score=(train=0.632, test=0.568) total time=   0.2s\n",
      "[CV 2/2; 126/162] START leaf_size=7, n_neighbors=9, p=2.........................\n",
      "[CV 2/2; 126/162] END leaf_size=7, n_neighbors=9, p=2;, score=(train=0.652, test=0.565) total time=   0.2s\n",
      "[CV 1/2; 127/162] START leaf_size=8, n_neighbors=1, p=1.........................\n",
      "[CV 1/2; 127/162] END leaf_size=8, n_neighbors=1, p=1;, score=(train=1.000, test=0.575) total time=   0.6s\n",
      "[CV 2/2; 127/162] START leaf_size=8, n_neighbors=1, p=1.........................\n",
      "[CV 2/2; 127/162] END leaf_size=8, n_neighbors=1, p=1;, score=(train=1.000, test=0.571) total time=   0.5s\n",
      "[CV 1/2; 128/162] START leaf_size=8, n_neighbors=1, p=2.........................\n",
      "[CV 1/2; 128/162] END leaf_size=8, n_neighbors=1, p=2;, score=(train=1.000, test=0.560) total time=   0.2s\n",
      "[CV 2/2; 128/162] START leaf_size=8, n_neighbors=1, p=2.........................\n",
      "[CV 2/2; 128/162] END leaf_size=8, n_neighbors=1, p=2;, score=(train=1.000, test=0.552) total time=   0.2s\n",
      "[CV 1/2; 129/162] START leaf_size=8, n_neighbors=2, p=1.........................\n",
      "[CV 1/2; 129/162] END leaf_size=8, n_neighbors=2, p=1;, score=(train=0.827, test=0.539) total time=   0.6s\n",
      "[CV 2/2; 129/162] START leaf_size=8, n_neighbors=2, p=1.........................\n",
      "[CV 2/2; 129/162] END leaf_size=8, n_neighbors=2, p=1;, score=(train=0.840, test=0.539) total time=   0.6s\n",
      "[CV 1/2; 130/162] START leaf_size=8, n_neighbors=2, p=2.........................\n",
      "[CV 1/2; 130/162] END leaf_size=8, n_neighbors=2, p=2;, score=(train=0.825, test=0.531) total time=   0.2s\n",
      "[CV 2/2; 130/162] START leaf_size=8, n_neighbors=2, p=2.........................\n",
      "[CV 2/2; 130/162] END leaf_size=8, n_neighbors=2, p=2;, score=(train=0.840, test=0.504) total time=   0.3s\n",
      "[CV 1/2; 131/162] START leaf_size=8, n_neighbors=3, p=1.........................\n",
      "[CV 1/2; 131/162] END leaf_size=8, n_neighbors=3, p=1;, score=(train=0.762, test=0.574) total time=   0.5s\n",
      "[CV 2/2; 131/162] START leaf_size=8, n_neighbors=3, p=1.........................\n",
      "[CV 2/2; 131/162] END leaf_size=8, n_neighbors=3, p=1;, score=(train=0.784, test=0.569) total time=   0.5s\n",
      "[CV 1/2; 132/162] START leaf_size=8, n_neighbors=3, p=2.........................\n",
      "[CV 1/2; 132/162] END leaf_size=8, n_neighbors=3, p=2;, score=(train=0.756, test=0.560) total time=   0.2s\n",
      "[CV 2/2; 132/162] START leaf_size=8, n_neighbors=3, p=2.........................\n",
      "[CV 2/2; 132/162] END leaf_size=8, n_neighbors=3, p=2;, score=(train=0.783, test=0.555) total time=   0.2s\n",
      "[CV 1/2; 133/162] START leaf_size=8, n_neighbors=4, p=1.........................\n",
      "[CV 1/2; 133/162] END leaf_size=8, n_neighbors=4, p=1;, score=(train=0.736, test=0.593) total time=   0.5s\n",
      "[CV 2/2; 133/162] START leaf_size=8, n_neighbors=4, p=1.........................\n",
      "[CV 2/2; 133/162] END leaf_size=8, n_neighbors=4, p=1;, score=(train=0.753, test=0.578) total time=   0.5s\n",
      "[CV 1/2; 134/162] START leaf_size=8, n_neighbors=4, p=2.........................\n",
      "[CV 1/2; 134/162] END leaf_size=8, n_neighbors=4, p=2;, score=(train=0.723, test=0.578) total time=   0.2s\n",
      "[CV 2/2; 134/162] START leaf_size=8, n_neighbors=4, p=2.........................\n",
      "[CV 2/2; 134/162] END leaf_size=8, n_neighbors=4, p=2;, score=(train=0.740, test=0.566) total time=   0.2s\n",
      "[CV 1/2; 135/162] START leaf_size=8, n_neighbors=5, p=1.........................\n",
      "[CV 1/2; 135/162] END leaf_size=8, n_neighbors=5, p=1;, score=(train=0.709, test=0.585) total time=   0.5s\n",
      "[CV 2/2; 135/162] START leaf_size=8, n_neighbors=5, p=1.........................\n",
      "[CV 2/2; 135/162] END leaf_size=8, n_neighbors=5, p=1;, score=(train=0.730, test=0.575) total time=   0.6s\n",
      "[CV 1/2; 136/162] START leaf_size=8, n_neighbors=5, p=2.........................\n",
      "[CV 1/2; 136/162] END leaf_size=8, n_neighbors=5, p=2;, score=(train=0.698, test=0.575) total time=   0.2s\n",
      "[CV 2/2; 136/162] START leaf_size=8, n_neighbors=5, p=2.........................\n",
      "[CV 2/2; 136/162] END leaf_size=8, n_neighbors=5, p=2;, score=(train=0.713, test=0.565) total time=   0.1s\n",
      "[CV 1/2; 137/162] START leaf_size=8, n_neighbors=6, p=1.........................\n",
      "[CV 1/2; 137/162] END leaf_size=8, n_neighbors=6, p=1;, score=(train=0.689, test=0.595) total time=   0.5s\n",
      "[CV 2/2; 137/162] START leaf_size=8, n_neighbors=6, p=1.........................\n",
      "[CV 2/2; 137/162] END leaf_size=8, n_neighbors=6, p=1;, score=(train=0.705, test=0.569) total time=   0.6s\n",
      "[CV 1/2; 138/162] START leaf_size=8, n_neighbors=6, p=2.........................\n",
      "[CV 1/2; 138/162] END leaf_size=8, n_neighbors=6, p=2;, score=(train=0.680, test=0.568) total time=   0.2s\n",
      "[CV 2/2; 138/162] START leaf_size=8, n_neighbors=6, p=2.........................\n",
      "[CV 2/2; 138/162] END leaf_size=8, n_neighbors=6, p=2;, score=(train=0.705, test=0.569) total time=   0.2s\n",
      "[CV 1/2; 139/162] START leaf_size=8, n_neighbors=7, p=1.........................\n",
      "[CV 1/2; 139/162] END leaf_size=8, n_neighbors=7, p=1;, score=(train=0.667, test=0.587) total time=   0.5s\n",
      "[CV 2/2; 139/162] START leaf_size=8, n_neighbors=7, p=1.........................\n",
      "[CV 2/2; 139/162] END leaf_size=8, n_neighbors=7, p=1;, score=(train=0.698, test=0.578) total time=   0.6s\n",
      "[CV 1/2; 140/162] START leaf_size=8, n_neighbors=7, p=2.........................\n",
      "[CV 1/2; 140/162] END leaf_size=8, n_neighbors=7, p=2;, score=(train=0.659, test=0.570) total time=   0.2s\n",
      "[CV 2/2; 140/162] START leaf_size=8, n_neighbors=7, p=2.........................\n",
      "[CV 2/2; 140/162] END leaf_size=8, n_neighbors=7, p=2;, score=(train=0.685, test=0.561) total time=   0.2s\n",
      "[CV 1/2; 141/162] START leaf_size=8, n_neighbors=8, p=1.........................\n",
      "[CV 1/2; 141/162] END leaf_size=8, n_neighbors=8, p=1;, score=(train=0.665, test=0.586) total time=   0.6s\n",
      "[CV 2/2; 141/162] START leaf_size=8, n_neighbors=8, p=1.........................\n",
      "[CV 2/2; 141/162] END leaf_size=8, n_neighbors=8, p=1;, score=(train=0.683, test=0.578) total time=   0.6s\n",
      "[CV 1/2; 142/162] START leaf_size=8, n_neighbors=8, p=2.........................\n",
      "[CV 1/2; 142/162] END leaf_size=8, n_neighbors=8, p=2;, score=(train=0.643, test=0.574) total time=   0.2s\n",
      "[CV 2/2; 142/162] START leaf_size=8, n_neighbors=8, p=2.........................\n",
      "[CV 2/2; 142/162] END leaf_size=8, n_neighbors=8, p=2;, score=(train=0.673, test=0.566) total time=   0.2s\n",
      "[CV 1/2; 143/162] START leaf_size=8, n_neighbors=9, p=1.........................\n",
      "[CV 1/2; 143/162] END leaf_size=8, n_neighbors=9, p=1;, score=(train=0.651, test=0.583) total time=   0.6s\n",
      "[CV 2/2; 143/162] START leaf_size=8, n_neighbors=9, p=1.........................\n",
      "[CV 2/2; 143/162] END leaf_size=8, n_neighbors=9, p=1;, score=(train=0.675, test=0.575) total time=   0.6s\n",
      "[CV 1/2; 144/162] START leaf_size=8, n_neighbors=9, p=2.........................\n",
      "[CV 1/2; 144/162] END leaf_size=8, n_neighbors=9, p=2;, score=(train=0.632, test=0.568) total time=   0.2s\n",
      "[CV 2/2; 144/162] START leaf_size=8, n_neighbors=9, p=2.........................\n",
      "[CV 2/2; 144/162] END leaf_size=8, n_neighbors=9, p=2;, score=(train=0.652, test=0.565) total time=   0.3s\n",
      "[CV 1/2; 145/162] START leaf_size=9, n_neighbors=1, p=1.........................\n",
      "[CV 1/2; 145/162] END leaf_size=9, n_neighbors=1, p=1;, score=(train=1.000, test=0.575) total time=   1.5s\n",
      "[CV 2/2; 145/162] START leaf_size=9, n_neighbors=1, p=1.........................\n",
      "[CV 2/2; 145/162] END leaf_size=9, n_neighbors=1, p=1;, score=(train=1.000, test=0.571) total time=   0.5s\n",
      "[CV 1/2; 146/162] START leaf_size=9, n_neighbors=1, p=2.........................\n",
      "[CV 1/2; 146/162] END leaf_size=9, n_neighbors=1, p=2;, score=(train=1.000, test=0.560) total time=   0.2s\n",
      "[CV 2/2; 146/162] START leaf_size=9, n_neighbors=1, p=2.........................\n",
      "[CV 2/2; 146/162] END leaf_size=9, n_neighbors=1, p=2;, score=(train=1.000, test=0.552) total time=   0.2s\n",
      "[CV 1/2; 147/162] START leaf_size=9, n_neighbors=2, p=1.........................\n",
      "[CV 1/2; 147/162] END leaf_size=9, n_neighbors=2, p=1;, score=(train=0.827, test=0.539) total time=   0.6s\n",
      "[CV 2/2; 147/162] START leaf_size=9, n_neighbors=2, p=1.........................\n",
      "[CV 2/2; 147/162] END leaf_size=9, n_neighbors=2, p=1;, score=(train=0.840, test=0.539) total time=   0.5s\n",
      "[CV 1/2; 148/162] START leaf_size=9, n_neighbors=2, p=2.........................\n",
      "[CV 1/2; 148/162] END leaf_size=9, n_neighbors=2, p=2;, score=(train=0.825, test=0.531) total time=   0.2s\n",
      "[CV 2/2; 148/162] START leaf_size=9, n_neighbors=2, p=2.........................\n",
      "[CV 2/2; 148/162] END leaf_size=9, n_neighbors=2, p=2;, score=(train=0.840, test=0.504) total time=   0.2s\n",
      "[CV 1/2; 149/162] START leaf_size=9, n_neighbors=3, p=1.........................\n",
      "[CV 1/2; 149/162] END leaf_size=9, n_neighbors=3, p=1;, score=(train=0.762, test=0.574) total time=   0.6s\n",
      "[CV 2/2; 149/162] START leaf_size=9, n_neighbors=3, p=1.........................\n",
      "[CV 2/2; 149/162] END leaf_size=9, n_neighbors=3, p=1;, score=(train=0.784, test=0.569) total time=   0.6s\n",
      "[CV 1/2; 150/162] START leaf_size=9, n_neighbors=3, p=2.........................\n",
      "[CV 1/2; 150/162] END leaf_size=9, n_neighbors=3, p=2;, score=(train=0.756, test=0.560) total time=   0.2s\n",
      "[CV 2/2; 150/162] START leaf_size=9, n_neighbors=3, p=2.........................\n",
      "[CV 2/2; 150/162] END leaf_size=9, n_neighbors=3, p=2;, score=(train=0.783, test=0.555) total time=   0.2s\n",
      "[CV 1/2; 151/162] START leaf_size=9, n_neighbors=4, p=1.........................\n",
      "[CV 1/2; 151/162] END leaf_size=9, n_neighbors=4, p=1;, score=(train=0.736, test=0.593) total time=   0.5s\n",
      "[CV 2/2; 151/162] START leaf_size=9, n_neighbors=4, p=1.........................\n",
      "[CV 2/2; 151/162] END leaf_size=9, n_neighbors=4, p=1;, score=(train=0.753, test=0.578) total time=   0.6s\n",
      "[CV 1/2; 152/162] START leaf_size=9, n_neighbors=4, p=2.........................\n",
      "[CV 1/2; 152/162] END leaf_size=9, n_neighbors=4, p=2;, score=(train=0.723, test=0.578) total time=   0.7s\n",
      "[CV 2/2; 152/162] START leaf_size=9, n_neighbors=4, p=2.........................\n",
      "[CV 2/2; 152/162] END leaf_size=9, n_neighbors=4, p=2;, score=(train=0.740, test=0.566) total time=   0.2s\n",
      "[CV 1/2; 153/162] START leaf_size=9, n_neighbors=5, p=1.........................\n",
      "[CV 1/2; 153/162] END leaf_size=9, n_neighbors=5, p=1;, score=(train=0.709, test=0.585) total time=   1.1s\n",
      "[CV 2/2; 153/162] START leaf_size=9, n_neighbors=5, p=1.........................\n",
      "[CV 2/2; 153/162] END leaf_size=9, n_neighbors=5, p=1;, score=(train=0.730, test=0.575) total time=   1.0s\n",
      "[CV 1/2; 154/162] START leaf_size=9, n_neighbors=5, p=2.........................\n",
      "[CV 1/2; 154/162] END leaf_size=9, n_neighbors=5, p=2;, score=(train=0.698, test=0.575) total time=   0.2s\n",
      "[CV 2/2; 154/162] START leaf_size=9, n_neighbors=5, p=2.........................\n",
      "[CV 2/2; 154/162] END leaf_size=9, n_neighbors=5, p=2;, score=(train=0.713, test=0.565) total time=   0.2s\n",
      "[CV 1/2; 155/162] START leaf_size=9, n_neighbors=6, p=1.........................\n",
      "[CV 1/2; 155/162] END leaf_size=9, n_neighbors=6, p=1;, score=(train=0.689, test=0.595) total time=   0.7s\n",
      "[CV 2/2; 155/162] START leaf_size=9, n_neighbors=6, p=1.........................\n",
      "[CV 2/2; 155/162] END leaf_size=9, n_neighbors=6, p=1;, score=(train=0.705, test=0.569) total time=   0.6s\n",
      "[CV 1/2; 156/162] START leaf_size=9, n_neighbors=6, p=2.........................\n",
      "[CV 1/2; 156/162] END leaf_size=9, n_neighbors=6, p=2;, score=(train=0.680, test=0.568) total time=   0.2s\n",
      "[CV 2/2; 156/162] START leaf_size=9, n_neighbors=6, p=2.........................\n",
      "[CV 2/2; 156/162] END leaf_size=9, n_neighbors=6, p=2;, score=(train=0.705, test=0.569) total time=   0.2s\n",
      "[CV 1/2; 157/162] START leaf_size=9, n_neighbors=7, p=1.........................\n",
      "[CV 1/2; 157/162] END leaf_size=9, n_neighbors=7, p=1;, score=(train=0.667, test=0.587) total time=   0.6s\n",
      "[CV 2/2; 157/162] START leaf_size=9, n_neighbors=7, p=1.........................\n",
      "[CV 2/2; 157/162] END leaf_size=9, n_neighbors=7, p=1;, score=(train=0.698, test=0.578) total time=   0.6s\n",
      "[CV 1/2; 158/162] START leaf_size=9, n_neighbors=7, p=2.........................\n",
      "[CV 1/2; 158/162] END leaf_size=9, n_neighbors=7, p=2;, score=(train=0.659, test=0.570) total time=   0.2s\n",
      "[CV 2/2; 158/162] START leaf_size=9, n_neighbors=7, p=2.........................\n",
      "[CV 2/2; 158/162] END leaf_size=9, n_neighbors=7, p=2;, score=(train=0.685, test=0.561) total time=   0.2s\n",
      "[CV 1/2; 159/162] START leaf_size=9, n_neighbors=8, p=1.........................\n",
      "[CV 1/2; 159/162] END leaf_size=9, n_neighbors=8, p=1;, score=(train=0.665, test=0.586) total time=   0.6s\n",
      "[CV 2/2; 159/162] START leaf_size=9, n_neighbors=8, p=1.........................\n",
      "[CV 2/2; 159/162] END leaf_size=9, n_neighbors=8, p=1;, score=(train=0.683, test=0.578) total time=   0.6s\n",
      "[CV 1/2; 160/162] START leaf_size=9, n_neighbors=8, p=2.........................\n",
      "[CV 1/2; 160/162] END leaf_size=9, n_neighbors=8, p=2;, score=(train=0.643, test=0.574) total time=   0.2s\n",
      "[CV 2/2; 160/162] START leaf_size=9, n_neighbors=8, p=2.........................\n",
      "[CV 2/2; 160/162] END leaf_size=9, n_neighbors=8, p=2;, score=(train=0.673, test=0.566) total time=   0.2s\n",
      "[CV 1/2; 161/162] START leaf_size=9, n_neighbors=9, p=1.........................\n",
      "[CV 1/2; 161/162] END leaf_size=9, n_neighbors=9, p=1;, score=(train=0.651, test=0.583) total time=   0.8s\n",
      "[CV 2/2; 161/162] START leaf_size=9, n_neighbors=9, p=1.........................\n",
      "[CV 2/2; 161/162] END leaf_size=9, n_neighbors=9, p=1;, score=(train=0.675, test=0.575) total time=   0.6s\n",
      "[CV 1/2; 162/162] START leaf_size=9, n_neighbors=9, p=2.........................\n",
      "[CV 1/2; 162/162] END leaf_size=9, n_neighbors=9, p=2;, score=(train=0.632, test=0.568) total time=   0.3s\n",
      "[CV 2/2; 162/162] START leaf_size=9, n_neighbors=9, p=2.........................\n",
      "[CV 2/2; 162/162] END leaf_size=9, n_neighbors=9, p=2;, score=(train=0.652, test=0.565) total time=   0.2s\n",
      "best parameters:  {'leaf_size': 1, 'n_neighbors': 4, 'p': 1}\n",
      "score:  0.5852083333333333\n",
      "['Rock' 'Rock' 'Country' 'Rock' 'Hip-Hop' 'Rock' 'Electronic' 'Electronic'\n",
      " 'Rock' 'Country']\n",
      "track_id\n",
      "17725            Rock\n",
      "9554             Rock\n",
      "4514     Experimental\n",
      "4801             Rock\n",
      "11800         Hip-Hop\n",
      "18687            Rock\n",
      "9091       Electronic\n",
      "12369         Hip-Hop\n",
      "10149            Rock\n",
      "486              Rock\n",
      "Name: genre_top, dtype: category\n",
      "Categories (16, object): ['Blues', 'Classical', 'Country', 'Easy Listening', ..., 'Pop', 'Rock', 'Soul-RnB', 'Spoken']\n",
      "       0    1     2    3     4     5     6    7     8     9    10   11    12  \\\n",
      "0     0.0  0.0  0.00  0.0  0.00  0.00  0.00  0.0  0.00  0.00  0.0  0.0  1.00   \n",
      "1     0.0  0.0  0.00  0.0  0.00  0.25  0.00  0.0  0.00  0.00  0.0  0.0  0.75   \n",
      "2     0.0  0.0  0.25  0.0  0.00  0.00  0.25  0.0  0.25  0.00  0.0  0.0  0.00   \n",
      "3     0.0  0.0  0.00  0.0  0.25  0.00  0.00  0.0  0.00  0.00  0.0  0.0  0.75   \n",
      "4     0.0  0.0  0.00  0.0  0.00  0.00  0.75  0.0  0.00  0.00  0.0  0.0  0.00   \n",
      "...   ...  ...   ...  ...   ...   ...   ...  ...   ...   ...  ...  ...   ...   \n",
      "3195  0.0  0.0  0.00  0.0  0.00  0.00  0.00  0.0  0.00  0.00  0.0  0.0  0.75   \n",
      "3196  0.0  0.0  0.00  0.0  0.00  0.00  0.00  0.0  0.00  0.00  0.0  0.0  1.00   \n",
      "3197  0.0  0.0  0.00  0.0  0.00  0.00  0.00  0.0  0.50  0.00  0.0  0.0  0.50   \n",
      "3198  0.0  0.0  0.00  0.0  0.00  0.00  0.00  0.0  0.00  0.00  0.0  0.0  1.00   \n",
      "3199  0.0  0.0  0.00  0.0  0.25  0.00  0.00  0.0  0.25  0.25  0.0  0.0  0.25   \n",
      "\n",
      "        13    14  \n",
      "0     0.00  0.00  \n",
      "1     0.00  0.00  \n",
      "2     0.00  0.25  \n",
      "3     0.00  0.00  \n",
      "4     0.00  0.25  \n",
      "...    ...   ...  \n",
      "3195  0.25  0.00  \n",
      "3196  0.00  0.00  \n",
      "3197  0.00  0.00  \n",
      "3198  0.00  0.00  \n",
      "3199  0.00  0.00  \n",
      "\n",
      "[3200 rows x 15 columns]\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "              Blues       0.32      0.29      0.30        21\n",
      "          Classical       0.55      0.97      0.71        63\n",
      "            Country       0.21      0.58      0.31        36\n",
      "         Electronic       0.60      0.40      0.48       464\n",
      "       Experimental       0.54      0.47      0.50       491\n",
      "               Folk       0.60      0.53      0.57       270\n",
      "            Hip-Hop       0.66      0.59      0.62       192\n",
      "       Instrumental       0.25      0.02      0.04        45\n",
      "      International       0.42      0.62      0.50        77\n",
      "               Jazz       0.57      0.42      0.48        74\n",
      "Old-Time / Historic       0.97      0.94      0.96        81\n",
      "                Pop       0.56      0.11      0.19        79\n",
      "               Rock       0.69      0.84      0.76      1236\n",
      "           Soul-RnB       0.83      0.28      0.42        18\n",
      "             Spoken       0.64      0.47      0.54        53\n",
      "\n",
      "           accuracy                           0.62      3200\n",
      "          macro avg       0.56      0.50      0.49      3200\n",
      "       weighted avg       0.62      0.62      0.61      3200\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "KNeighborsClassifier(leaf_size=1, n_neighbors=4, p=1)",
      "text/html": "<style>#sk-container-id-12 {color: black;background-color: white;}#sk-container-id-12 pre{padding: 0;}#sk-container-id-12 div.sk-toggleable {background-color: white;}#sk-container-id-12 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-12 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-12 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-12 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-12 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-12 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-12 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-12 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-12 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-12 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-12 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-12 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-12 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-12 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-12 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-12 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-12 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-12 div.sk-item {position: relative;z-index: 1;}#sk-container-id-12 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-12 div.sk-item::before, #sk-container-id-12 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-12 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-12 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-12 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-12 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-12 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-12 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-12 div.sk-label-container {text-align: center;}#sk-container-id-12 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-12 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-12\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KNeighborsClassifier(leaf_size=1, n_neighbors=4, p=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-12\" type=\"checkbox\" checked><label for=\"sk-estimator-id-12\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KNeighborsClassifier</label><div class=\"sk-toggleable__content\"><pre>KNeighborsClassifier(leaf_size=1, n_neighbors=4, p=1)</pre></div></div></div></div></div>"
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# KNN model evaluation\n",
    "leaf_size = list(range(1,10))\n",
    "n_neighbors = list(range(1,10))\n",
    "p=[1,2]\n",
    "param_evaluated = dict(leaf_size=leaf_size, n_neighbors=n_neighbors, p=p)\n",
    "\n",
    "knn_base_model = KNeighborsClassifier()\n",
    "knn_best_model = optimize_model(knn_base_model, X_train, y_train, param_evaluated)\n",
    "evaluate_classifier(X_train, X_test, y_train, y_test, knn_best_model)\n",
    "knn_best_model\n",
    "\n",
    "# After run\n",
    "# best parameters:  {'leaf_size': 1, 'n_neighbors': 4, 'p': 1}\n",
    "# score:  0.5852083333333333"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h1>Random forest</h1>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Rock' 'Rock' 'Electronic' 'Rock' 'Hip-Hop' 'Rock' 'Electronic' 'Hip-Hop'\n",
      " 'Rock' 'Rock']\n",
      "track_id\n",
      "17725            Rock\n",
      "9554             Rock\n",
      "4514     Experimental\n",
      "4801             Rock\n",
      "11800         Hip-Hop\n",
      "18687            Rock\n",
      "9091       Electronic\n",
      "12369         Hip-Hop\n",
      "10149            Rock\n",
      "486              Rock\n",
      "Name: genre_top, dtype: category\n",
      "Categories (16, object): ['Blues', 'Classical', 'Country', 'Easy Listening', ..., 'Pop', 'Rock', 'Soul-RnB', 'Spoken']\n",
      "        0     1     2     3     4     5     6     7     8     9     10    11  \\\n",
      "0     0.00  0.00  0.02  0.02  0.29  0.01  0.02  0.03  0.01  0.00  0.00  0.03   \n",
      "1     0.02  0.00  0.03  0.11  0.12  0.09  0.04  0.00  0.04  0.02  0.00  0.06   \n",
      "2     0.01  0.00  0.01  0.23  0.21  0.03  0.06  0.02  0.05  0.05  0.00  0.03   \n",
      "3     0.00  0.00  0.00  0.02  0.02  0.00  0.00  0.00  0.00  0.00  0.00  0.01   \n",
      "4     0.00  0.00  0.02  0.20  0.11  0.03  0.44  0.00  0.02  0.00  0.00  0.04   \n",
      "...    ...   ...   ...   ...   ...   ...   ...   ...   ...   ...   ...   ...   \n",
      "3195  0.01  0.00  0.00  0.37  0.12  0.02  0.05  0.02  0.01  0.01  0.00  0.01   \n",
      "3196  0.00  0.00  0.02  0.01  0.02  0.00  0.00  0.00  0.01  0.00  0.00  0.01   \n",
      "3197  0.00  0.01  0.03  0.05  0.06  0.06  0.02  0.00  0.15  0.02  0.01  0.06   \n",
      "3198  0.00  0.00  0.01  0.06  0.01  0.02  0.00  0.00  0.01  0.00  0.01  0.04   \n",
      "3199  0.00  0.00  0.00  0.15  0.31  0.02  0.06  0.02  0.03  0.03  0.00  0.06   \n",
      "\n",
      "        12    13    14  \n",
      "0     0.57  0.00  0.00  \n",
      "1     0.46  0.00  0.01  \n",
      "2     0.19  0.00  0.11  \n",
      "3     0.95  0.00  0.00  \n",
      "4     0.13  0.00  0.01  \n",
      "...    ...   ...   ...  \n",
      "3195  0.36  0.02  0.00  \n",
      "3196  0.93  0.00  0.00  \n",
      "3197  0.52  0.00  0.01  \n",
      "3198  0.84  0.00  0.00  \n",
      "3199  0.28  0.00  0.04  \n",
      "\n",
      "[3200 rows x 15 columns]\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "              Blues       1.00      0.05      0.09        21\n",
      "          Classical       0.89      0.75      0.81        63\n",
      "            Country       0.00      0.00      0.00        36\n",
      "         Electronic       0.57      0.55      0.56       464\n",
      "       Experimental       0.57      0.48      0.52       491\n",
      "               Folk       0.56      0.64      0.60       270\n",
      "            Hip-Hop       0.81      0.56      0.66       192\n",
      "       Instrumental       1.00      0.04      0.09        45\n",
      "      International       0.92      0.14      0.25        77\n",
      "               Jazz       0.83      0.07      0.13        74\n",
      "Old-Time / Historic       0.95      0.94      0.94        81\n",
      "                Pop       1.00      0.01      0.02        79\n",
      "               Rock       0.65      0.90      0.75      1236\n",
      "           Soul-RnB       0.00      0.00      0.00        18\n",
      "             Spoken       0.62      0.25      0.35        53\n",
      "\n",
      "           accuracy                           0.64      3200\n",
      "          macro avg       0.69      0.36      0.38      3200\n",
      "       weighted avg       0.65      0.64      0.60      3200\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marco/repo/ML/venv/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/marco/repo/ML/venv/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/marco/repo/ML/venv/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_model = RandomForestClassifier()\n",
    "evaluate_classifier(X_train, X_test, y_train, y_test, rf_model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 22 candidates, totalling 44 fits\n",
      "[CV 1/2; 1/22] START criterion=gini, n_estimators=90............................\n",
      "[CV 1/2; 1/22] END criterion=gini, n_estimators=90;, score=(train=1.000, test=0.623) total time=   4.0s\n",
      "[CV 2/2; 1/22] START criterion=gini, n_estimators=90............................\n",
      "[CV 2/2; 1/22] END criterion=gini, n_estimators=90;, score=(train=1.000, test=0.607) total time=   3.5s\n",
      "[CV 1/2; 2/22] START criterion=gini, n_estimators=91............................\n",
      "[CV 1/2; 2/22] END criterion=gini, n_estimators=91;, score=(train=1.000, test=0.626) total time=   3.4s\n",
      "[CV 2/2; 2/22] START criterion=gini, n_estimators=91............................\n",
      "[CV 2/2; 2/22] END criterion=gini, n_estimators=91;, score=(train=1.000, test=0.603) total time=   3.4s\n",
      "[CV 1/2; 3/22] START criterion=gini, n_estimators=92............................\n",
      "[CV 1/2; 3/22] END criterion=gini, n_estimators=92;, score=(train=1.000, test=0.630) total time=   3.5s\n",
      "[CV 2/2; 3/22] START criterion=gini, n_estimators=92............................\n",
      "[CV 2/2; 3/22] END criterion=gini, n_estimators=92;, score=(train=1.000, test=0.600) total time=   3.5s\n",
      "[CV 1/2; 4/22] START criterion=gini, n_estimators=93............................\n",
      "[CV 1/2; 4/22] END criterion=gini, n_estimators=93;, score=(train=1.000, test=0.637) total time=   3.5s\n",
      "[CV 2/2; 4/22] START criterion=gini, n_estimators=93............................\n",
      "[CV 2/2; 4/22] END criterion=gini, n_estimators=93;, score=(train=1.000, test=0.610) total time=   3.4s\n",
      "[CV 1/2; 5/22] START criterion=gini, n_estimators=94............................\n",
      "[CV 1/2; 5/22] END criterion=gini, n_estimators=94;, score=(train=1.000, test=0.622) total time=   3.5s\n",
      "[CV 2/2; 5/22] START criterion=gini, n_estimators=94............................\n",
      "[CV 2/2; 5/22] END criterion=gini, n_estimators=94;, score=(train=1.000, test=0.607) total time=   3.5s\n",
      "[CV 1/2; 6/22] START criterion=gini, n_estimators=95............................\n",
      "[CV 1/2; 6/22] END criterion=gini, n_estimators=95;, score=(train=1.000, test=0.628) total time=   3.6s\n",
      "[CV 2/2; 6/22] START criterion=gini, n_estimators=95............................\n",
      "[CV 2/2; 6/22] END criterion=gini, n_estimators=95;, score=(train=1.000, test=0.610) total time=   3.5s\n",
      "[CV 1/2; 7/22] START criterion=gini, n_estimators=96............................\n",
      "[CV 1/2; 7/22] END criterion=gini, n_estimators=96;, score=(train=1.000, test=0.627) total time=   3.6s\n",
      "[CV 2/2; 7/22] START criterion=gini, n_estimators=96............................\n",
      "[CV 2/2; 7/22] END criterion=gini, n_estimators=96;, score=(train=1.000, test=0.608) total time=   3.6s\n",
      "[CV 1/2; 8/22] START criterion=gini, n_estimators=97............................\n",
      "[CV 1/2; 8/22] END criterion=gini, n_estimators=97;, score=(train=1.000, test=0.623) total time=   3.9s\n",
      "[CV 2/2; 8/22] START criterion=gini, n_estimators=97............................\n",
      "[CV 2/2; 8/22] END criterion=gini, n_estimators=97;, score=(train=1.000, test=0.610) total time=   4.0s\n",
      "[CV 1/2; 9/22] START criterion=gini, n_estimators=98............................\n",
      "[CV 1/2; 9/22] END criterion=gini, n_estimators=98;, score=(train=1.000, test=0.630) total time=   3.8s\n",
      "[CV 2/2; 9/22] START criterion=gini, n_estimators=98............................\n",
      "[CV 2/2; 9/22] END criterion=gini, n_estimators=98;, score=(train=1.000, test=0.611) total time=   3.8s\n",
      "[CV 1/2; 10/22] START criterion=gini, n_estimators=99...........................\n",
      "[CV 1/2; 10/22] END criterion=gini, n_estimators=99;, score=(train=1.000, test=0.624) total time=   3.9s\n",
      "[CV 2/2; 10/22] START criterion=gini, n_estimators=99...........................\n",
      "[CV 2/2; 10/22] END criterion=gini, n_estimators=99;, score=(train=1.000, test=0.611) total time=   3.8s\n",
      "[CV 1/2; 11/22] START criterion=gini, n_estimators=100..........................\n",
      "[CV 1/2; 11/22] END criterion=gini, n_estimators=100;, score=(train=1.000, test=0.624) total time=   4.0s\n",
      "[CV 2/2; 11/22] START criterion=gini, n_estimators=100..........................\n",
      "[CV 2/2; 11/22] END criterion=gini, n_estimators=100;, score=(train=1.000, test=0.605) total time=   3.8s\n",
      "[CV 1/2; 12/22] START criterion=entropy, n_estimators=90........................\n",
      "[CV 1/2; 12/22] END criterion=entropy, n_estimators=90;, score=(train=1.000, test=0.615) total time=   6.9s\n",
      "[CV 2/2; 12/22] START criterion=entropy, n_estimators=90........................\n",
      "[CV 2/2; 12/22] END criterion=entropy, n_estimators=90;, score=(train=1.000, test=0.607) total time=   7.0s\n",
      "[CV 1/2; 13/22] START criterion=entropy, n_estimators=91........................\n",
      "[CV 1/2; 13/22] END criterion=entropy, n_estimators=91;, score=(train=1.000, test=0.618) total time=   6.9s\n",
      "[CV 2/2; 13/22] START criterion=entropy, n_estimators=91........................\n",
      "[CV 2/2; 13/22] END criterion=entropy, n_estimators=91;, score=(train=1.000, test=0.607) total time=   7.0s\n",
      "[CV 1/2; 14/22] START criterion=entropy, n_estimators=92........................\n",
      "[CV 1/2; 14/22] END criterion=entropy, n_estimators=92;, score=(train=1.000, test=0.614) total time=   7.1s\n",
      "[CV 2/2; 14/22] START criterion=entropy, n_estimators=92........................\n",
      "[CV 2/2; 14/22] END criterion=entropy, n_estimators=92;, score=(train=1.000, test=0.595) total time=   7.1s\n",
      "[CV 1/2; 15/22] START criterion=entropy, n_estimators=93........................\n",
      "[CV 1/2; 15/22] END criterion=entropy, n_estimators=93;, score=(train=1.000, test=0.620) total time=   7.2s\n",
      "[CV 2/2; 15/22] START criterion=entropy, n_estimators=93........................\n",
      "[CV 2/2; 15/22] END criterion=entropy, n_estimators=93;, score=(train=1.000, test=0.611) total time=   7.1s\n",
      "[CV 1/2; 16/22] START criterion=entropy, n_estimators=94........................\n",
      "[CV 1/2; 16/22] END criterion=entropy, n_estimators=94;, score=(train=1.000, test=0.618) total time=   7.3s\n",
      "[CV 2/2; 16/22] START criterion=entropy, n_estimators=94........................\n",
      "[CV 2/2; 16/22] END criterion=entropy, n_estimators=94;, score=(train=1.000, test=0.603) total time=   7.2s\n",
      "[CV 1/2; 17/22] START criterion=entropy, n_estimators=95........................\n",
      "[CV 1/2; 17/22] END criterion=entropy, n_estimators=95;, score=(train=1.000, test=0.621) total time=   7.3s\n",
      "[CV 2/2; 17/22] START criterion=entropy, n_estimators=95........................\n",
      "[CV 2/2; 17/22] END criterion=entropy, n_estimators=95;, score=(train=1.000, test=0.603) total time=   7.3s\n",
      "[CV 1/2; 18/22] START criterion=entropy, n_estimators=96........................\n",
      "[CV 1/2; 18/22] END criterion=entropy, n_estimators=96;, score=(train=1.000, test=0.624) total time=   7.4s\n",
      "[CV 2/2; 18/22] START criterion=entropy, n_estimators=96........................\n",
      "[CV 2/2; 18/22] END criterion=entropy, n_estimators=96;, score=(train=1.000, test=0.603) total time=   7.3s\n",
      "[CV 1/2; 19/22] START criterion=entropy, n_estimators=97........................\n",
      "[CV 1/2; 19/22] END criterion=entropy, n_estimators=97;, score=(train=1.000, test=0.616) total time=   7.4s\n",
      "[CV 2/2; 19/22] START criterion=entropy, n_estimators=97........................\n",
      "[CV 2/2; 19/22] END criterion=entropy, n_estimators=97;, score=(train=1.000, test=0.608) total time=   7.6s\n",
      "[CV 1/2; 20/22] START criterion=entropy, n_estimators=98........................\n",
      "[CV 1/2; 20/22] END criterion=entropy, n_estimators=98;, score=(train=1.000, test=0.621) total time=   8.5s\n",
      "[CV 2/2; 20/22] START criterion=entropy, n_estimators=98........................\n",
      "[CV 2/2; 20/22] END criterion=entropy, n_estimators=98;, score=(train=1.000, test=0.612) total time=   8.4s\n",
      "[CV 1/2; 21/22] START criterion=entropy, n_estimators=99........................\n",
      "[CV 1/2; 21/22] END criterion=entropy, n_estimators=99;, score=(train=1.000, test=0.621) total time=   7.9s\n",
      "[CV 2/2; 21/22] START criterion=entropy, n_estimators=99........................\n",
      "[CV 2/2; 21/22] END criterion=entropy, n_estimators=99;, score=(train=1.000, test=0.613) total time=   7.9s\n",
      "[CV 1/2; 22/22] START criterion=entropy, n_estimators=100.......................\n",
      "[CV 1/2; 22/22] END criterion=entropy, n_estimators=100;, score=(train=1.000, test=0.621) total time=   7.8s\n",
      "[CV 2/2; 22/22] START criterion=entropy, n_estimators=100.......................\n",
      "[CV 2/2; 22/22] END criterion=entropy, n_estimators=100;, score=(train=1.000, test=0.607) total time=   7.8s\n",
      "best parameters:  {'criterion': 'gini', 'n_estimators': 93}\n",
      "score:  0.623125\n",
      "['Rock' 'Rock' 'Electronic' 'Rock' 'Hip-Hop' 'Rock' 'Electronic' 'Hip-Hop'\n",
      " 'Rock' 'Electronic']\n",
      "track_id\n",
      "17725            Rock\n",
      "9554             Rock\n",
      "4514     Experimental\n",
      "4801             Rock\n",
      "11800         Hip-Hop\n",
      "18687            Rock\n",
      "9091       Electronic\n",
      "12369         Hip-Hop\n",
      "10149            Rock\n",
      "486              Rock\n",
      "Name: genre_top, dtype: category\n",
      "Categories (16, object): ['Blues', 'Classical', 'Country', 'Easy Listening', ..., 'Pop', 'Rock', 'Soul-RnB', 'Spoken']\n",
      "            0         1         2         3         4         5         6   \\\n",
      "0     0.000000  0.000000  0.032258  0.086022  0.172043  0.010753  0.010753   \n",
      "1     0.010753  0.000000  0.021505  0.096774  0.075269  0.032258  0.096774   \n",
      "2     0.000000  0.000000  0.010753  0.236559  0.161290  0.053763  0.161290   \n",
      "3     0.000000  0.000000  0.021505  0.032258  0.032258  0.000000  0.000000   \n",
      "4     0.000000  0.010753  0.010753  0.161290  0.032258  0.021505  0.462366   \n",
      "...        ...       ...       ...       ...       ...       ...       ...   \n",
      "3195  0.000000  0.000000  0.010753  0.344086  0.086022  0.032258  0.021505   \n",
      "3196  0.000000  0.000000  0.010753  0.000000  0.010753  0.010753  0.010753   \n",
      "3197  0.000000  0.000000  0.021505  0.010753  0.086022  0.043011  0.010753   \n",
      "3198  0.000000  0.000000  0.021505  0.032258  0.010753  0.043011  0.000000   \n",
      "3199  0.010753  0.021505  0.000000  0.129032  0.268817  0.043011  0.021505   \n",
      "\n",
      "            7         8         9    10        11        12        13  \\\n",
      "0     0.053763  0.000000  0.010753  0.0  0.021505  0.591398  0.000000   \n",
      "1     0.010753  0.021505  0.010753  0.0  0.053763  0.537634  0.010753   \n",
      "2     0.043011  0.032258  0.010753  0.0  0.021505  0.161290  0.000000   \n",
      "3     0.000000  0.000000  0.000000  0.0  0.000000  0.903226  0.000000   \n",
      "4     0.010753  0.000000  0.000000  0.0  0.032258  0.247312  0.010753   \n",
      "...        ...       ...       ...  ...       ...       ...       ...   \n",
      "3195  0.000000  0.000000  0.000000  0.0  0.021505  0.473118  0.010753   \n",
      "3196  0.010753  0.010753  0.000000  0.0  0.000000  0.924731  0.000000   \n",
      "3197  0.000000  0.215054  0.000000  0.0  0.064516  0.548387  0.000000   \n",
      "3198  0.000000  0.010753  0.010753  0.0  0.010753  0.849462  0.010753   \n",
      "3199  0.043011  0.043011  0.064516  0.0  0.043011  0.258065  0.010753   \n",
      "\n",
      "            14  \n",
      "0     0.010753  \n",
      "1     0.021505  \n",
      "2     0.107527  \n",
      "3     0.010753  \n",
      "4     0.000000  \n",
      "...        ...  \n",
      "3195  0.000000  \n",
      "3196  0.010753  \n",
      "3197  0.000000  \n",
      "3198  0.000000  \n",
      "3199  0.043011  \n",
      "\n",
      "[3200 rows x 15 columns]\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "              Blues       1.00      0.05      0.09        21\n",
      "          Classical       0.87      0.76      0.81        63\n",
      "            Country       0.00      0.00      0.00        36\n",
      "         Electronic       0.55      0.53      0.54       464\n",
      "       Experimental       0.56      0.48      0.51       491\n",
      "               Folk       0.56      0.65      0.60       270\n",
      "            Hip-Hop       0.79      0.54      0.64       192\n",
      "       Instrumental       0.67      0.04      0.08        45\n",
      "      International       1.00      0.12      0.21        77\n",
      "               Jazz       1.00      0.05      0.10        74\n",
      "Old-Time / Historic       0.93      0.94      0.93        81\n",
      "                Pop       0.00      0.00      0.00        79\n",
      "               Rock       0.64      0.89      0.75      1236\n",
      "           Soul-RnB       0.00      0.00      0.00        18\n",
      "             Spoken       0.61      0.21      0.31        53\n",
      "\n",
      "           accuracy                           0.63      3200\n",
      "          macro avg       0.61      0.35      0.37      3200\n",
      "       weighted avg       0.62      0.63      0.59      3200\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marco/repo/ML/venv/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/marco/repo/ML/venv/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/marco/repo/ML/venv/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": "RandomForestClassifier(n_estimators=93)",
      "text/html": "<style>#sk-container-id-14 {color: black;background-color: white;}#sk-container-id-14 pre{padding: 0;}#sk-container-id-14 div.sk-toggleable {background-color: white;}#sk-container-id-14 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-14 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-14 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-14 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-14 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-14 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-14 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-14 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-14 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-14 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-14 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-14 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-14 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-14 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-14 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-14 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-14 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-14 div.sk-item {position: relative;z-index: 1;}#sk-container-id-14 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-14 div.sk-item::before, #sk-container-id-14 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-14 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-14 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-14 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-14 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-14 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-14 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-14 div.sk-label-container {text-align: center;}#sk-container-id-14 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-14 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-14\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(n_estimators=93)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-14\" type=\"checkbox\" checked><label for=\"sk-estimator-id-14\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(n_estimators=93)</pre></div></div></div></div></div>"
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# random forest model evaluation\n",
    "n_estimators = list(range(90, 101))\n",
    "\n",
    "param_evaluated = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'n_estimators': n_estimators}\n",
    "\n",
    "\"\"\"param_evaluated = {\n",
    "    'bootstrap': [True],\n",
    "    'max_depth': [80, 90, 100, 110],\n",
    "    'max_features': [2, 3],\n",
    "    'min_samples_leaf': [3, 4, 5],\n",
    "    'min_samples_split': [8, 10, 12],\n",
    "    'n_estimators': [100, 200, 300, 1000]\n",
    "}\"\"\"\n",
    "\n",
    "rf_base_model = RandomForestClassifier()\n",
    "rf_best_model = optimize_model(rf_base_model, X_train, y_train, param_evaluated)\n",
    "evaluate_classifier(X_train, X_test, y_train, y_test, rf_best_model)\n",
    "rf_best_model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h1>Naive bayes</h1>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Instrumental' 'Country' 'Spoken' 'Pop' 'Hip-Hop' 'Pop' 'Electronic'\n",
      " 'Hip-Hop' 'Pop' 'Pop']\n",
      "track_id\n",
      "17725            Rock\n",
      "9554             Rock\n",
      "4514     Experimental\n",
      "4801             Rock\n",
      "11800         Hip-Hop\n",
      "18687            Rock\n",
      "9091       Electronic\n",
      "12369         Hip-Hop\n",
      "10149            Rock\n",
      "486              Rock\n",
      "Name: genre_top, dtype: category\n",
      "Categories (16, object): ['Blues', 'Classical', 'Country', 'Easy Listening', ..., 'Pop', 'Rock', 'Soul-RnB', 'Spoken']\n",
      "                 0              1              2              3   \\\n",
      "0      0.000000e+00   0.000000e+00   0.000000e+00   4.045735e-85   \n",
      "1      3.042049e-18  4.064874e-210   1.000000e+00  1.334857e-202   \n",
      "2      9.993849e-63  4.613704e-150   3.235409e-50  8.472725e-224   \n",
      "3     2.515214e-157   0.000000e+00   2.004757e-34  2.588808e-153   \n",
      "4      3.988380e-86   0.000000e+00   0.000000e+00  3.488739e-207   \n",
      "...             ...            ...            ...            ...   \n",
      "3195   0.000000e+00   0.000000e+00   9.389734e-65  9.511485e-177   \n",
      "3196   0.000000e+00   0.000000e+00   2.703119e-03  1.115318e-173   \n",
      "3197   0.000000e+00   0.000000e+00  8.506960e-117  7.740767e-171   \n",
      "3198   4.446748e-20  2.626219e-214   1.000000e+00  4.845005e-220   \n",
      "3199   5.280077e-39  3.852200e-173   5.430199e-07  9.220246e-210   \n",
      "\n",
      "                 4              5              6              7   \\\n",
      "0      1.349723e-49  6.561744e-131  2.565978e-207   9.999074e-01   \n",
      "1     1.759431e-164  1.368545e-105   1.783788e-28  8.716225e-129   \n",
      "2     9.114325e-191  4.724201e-159   8.789002e-38  1.249057e-168   \n",
      "3     2.542582e-109   4.153962e-99   2.663189e-01   1.315418e-78   \n",
      "4     6.361973e-174  1.118897e-262   1.000000e+00   0.000000e+00   \n",
      "...             ...            ...            ...            ...   \n",
      "3195  3.405099e-147  3.370482e-148   7.564684e-22  2.323566e-111   \n",
      "3196  4.050998e-129   3.552693e-96   2.111644e-01   5.009123e-97   \n",
      "3197  6.940533e-116   9.438750e-76   8.971772e-33   5.868658e-80   \n",
      "3198  4.759768e-172  3.203350e-122   1.252019e-71  2.435922e-141   \n",
      "3199  7.404422e-163  3.278017e-120   8.620045e-22  3.833969e-134   \n",
      "\n",
      "                 8              9              10            11  \\\n",
      "0     3.621497e-285  1.888427e-228   0.000000e+00  1.435972e-20   \n",
      "1      3.766780e-31   2.639918e-41  1.220342e-321  2.015839e-26   \n",
      "2      2.348775e-40   9.601947e-33   0.000000e+00  8.922350e-74   \n",
      "3      4.978321e-87  1.466733e-113   0.000000e+00  7.336803e-01   \n",
      "4      0.000000e+00  6.939171e-129   0.000000e+00  1.036411e-44   \n",
      "...             ...            ...            ...           ...   \n",
      "3195   1.306234e-54   1.968215e-54   0.000000e+00  5.975303e-29   \n",
      "3196   1.035111e-33   1.583145e-59  1.260112e-309  7.861325e-01   \n",
      "3197   8.783443e-14   1.535391e-45  1.118591e-167  1.000000e+00   \n",
      "3198   6.885200e-42   1.931033e-49  3.916872e-189  4.594741e-42   \n",
      "3199   1.099586e-12   5.117242e-21   1.023221e-93  1.602228e-40   \n",
      "\n",
      "                 12            13             14  \n",
      "0      9.260521e-05  0.000000e+00  6.078763e-244  \n",
      "1      6.601318e-70  1.932268e-28   3.209217e-52  \n",
      "2     2.953155e-132  5.557300e-29   1.000000e+00  \n",
      "3      8.205503e-07  0.000000e+00   4.723353e-72  \n",
      "4      1.896759e-79  0.000000e+00   1.603805e-61  \n",
      "...             ...           ...            ...  \n",
      "3195   7.741660e-52  1.000000e+00   4.402268e-60  \n",
      "3196   5.318850e-17  1.049729e-37   2.917004e-24  \n",
      "3197   1.019917e-20  0.000000e+00   5.523918e-32  \n",
      "3198   3.512997e-60  1.898575e-36   1.782067e-59  \n",
      "3199   1.453989e-75  7.249890e-15   9.999995e-01  \n",
      "\n",
      "[3200 rows x 15 columns]\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "              Blues       0.03      0.29      0.05        21\n",
      "          Classical       0.69      0.70      0.69        63\n",
      "            Country       0.07      0.56      0.12        36\n",
      "         Electronic       0.73      0.13      0.22       464\n",
      "       Experimental       0.47      0.04      0.08       491\n",
      "               Folk       0.39      0.10      0.16       270\n",
      "            Hip-Hop       0.28      0.65      0.39       192\n",
      "       Instrumental       0.07      0.20      0.11        45\n",
      "      International       0.12      0.39      0.18        77\n",
      "               Jazz       0.10      0.32      0.16        74\n",
      "Old-Time / Historic       0.69      0.91      0.78        81\n",
      "                Pop       0.02      0.16      0.04        79\n",
      "               Rock       0.65      0.12      0.20      1236\n",
      "           Soul-RnB       0.05      0.56      0.09        18\n",
      "             Spoken       0.13      0.77      0.23        53\n",
      "\n",
      "           accuracy                           0.20      3200\n",
      "          macro avg       0.30      0.39      0.23      3200\n",
      "       weighted avg       0.52      0.20      0.21      3200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "nb_model = GaussianNB()\n",
    "evaluate_classifier(X_train, X_test, y_train, y_test, nb_model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 14 candidates, totalling 28 fits\n",
      "[CV 1/2; 1/14] START var_smoothing=0.01.........................................\n",
      "[CV 1/2; 1/14] END var_smoothing=0.01;, score=(train=0.435, test=0.400) total time=   0.1s\n",
      "[CV 2/2; 1/14] START var_smoothing=0.01.........................................\n",
      "[CV 2/2; 1/14] END var_smoothing=0.01;, score=(train=0.455, test=0.398) total time=   0.1s\n",
      "[CV 1/2; 2/14] START var_smoothing=0.001........................................\n",
      "[CV 1/2; 2/14] END var_smoothing=0.001;, score=(train=0.424, test=0.381) total time=   0.1s\n",
      "[CV 2/2; 2/14] START var_smoothing=0.001........................................\n",
      "[CV 2/2; 2/14] END var_smoothing=0.001;, score=(train=0.444, test=0.381) total time=   0.1s\n",
      "[CV 1/2; 3/14] START var_smoothing=0.0001.......................................\n",
      "[CV 1/2; 3/14] END var_smoothing=0.0001;, score=(train=0.404, test=0.350) total time=   0.1s\n",
      "[CV 2/2; 3/14] START var_smoothing=0.0001.......................................\n",
      "[CV 2/2; 3/14] END var_smoothing=0.0001;, score=(train=0.431, test=0.367) total time=   0.1s\n",
      "[CV 1/2; 4/14] START var_smoothing=1e-05........................................\n",
      "[CV 1/2; 4/14] END var_smoothing=1e-05;, score=(train=0.375, test=0.325) total time=   0.1s\n",
      "[CV 2/2; 4/14] START var_smoothing=1e-05........................................\n",
      "[CV 2/2; 4/14] END var_smoothing=1e-05;, score=(train=0.411, test=0.354) total time=   0.1s\n",
      "[CV 1/2; 5/14] START var_smoothing=1e-06........................................\n",
      "[CV 1/2; 5/14] END var_smoothing=1e-06;, score=(train=0.347, test=0.298) total time=   0.1s\n",
      "[CV 2/2; 5/14] START var_smoothing=1e-06........................................\n",
      "[CV 2/2; 5/14] END var_smoothing=1e-06;, score=(train=0.399, test=0.335) total time=   0.1s\n",
      "[CV 1/2; 6/14] START var_smoothing=1e-07........................................\n",
      "[CV 1/2; 6/14] END var_smoothing=1e-07;, score=(train=0.316, test=0.267) total time=   0.1s\n",
      "[CV 2/2; 6/14] START var_smoothing=1e-07........................................\n",
      "[CV 2/2; 6/14] END var_smoothing=1e-07;, score=(train=0.380, test=0.325) total time=   0.1s\n",
      "[CV 1/2; 7/14] START var_smoothing=1e-08........................................\n",
      "[CV 1/2; 7/14] END var_smoothing=1e-08;, score=(train=0.290, test=0.245) total time=   0.1s\n",
      "[CV 2/2; 7/14] START var_smoothing=1e-08........................................\n",
      "[CV 2/2; 7/14] END var_smoothing=1e-08;, score=(train=0.362, test=0.307) total time=   0.1s\n",
      "[CV 1/2; 8/14] START var_smoothing=1e-09........................................\n",
      "[CV 1/2; 8/14] END var_smoothing=1e-09;, score=(train=0.270, test=0.222) total time=   0.1s\n",
      "[CV 2/2; 8/14] START var_smoothing=1e-09........................................\n",
      "[CV 2/2; 8/14] END var_smoothing=1e-09;, score=(train=0.347, test=0.293) total time=   0.1s\n",
      "[CV 1/2; 9/14] START var_smoothing=1e-10........................................\n",
      "[CV 1/2; 9/14] END var_smoothing=1e-10;, score=(train=0.252, test=0.209) total time=   0.1s\n",
      "[CV 2/2; 9/14] START var_smoothing=1e-10........................................\n",
      "[CV 2/2; 9/14] END var_smoothing=1e-10;, score=(train=0.337, test=0.280) total time=   0.1s\n",
      "[CV 1/2; 10/14] START var_smoothing=1e-11.......................................\n",
      "[CV 1/2; 10/14] END var_smoothing=1e-11;, score=(train=0.245, test=0.199) total time=   0.1s\n",
      "[CV 2/2; 10/14] START var_smoothing=1e-11.......................................\n",
      "[CV 2/2; 10/14] END var_smoothing=1e-11;, score=(train=0.319, test=0.264) total time=   0.1s\n",
      "[CV 1/2; 11/14] START var_smoothing=1e-12.......................................\n",
      "[CV 1/2; 11/14] END var_smoothing=1e-12;, score=(train=0.235, test=0.192) total time=   0.1s\n",
      "[CV 2/2; 11/14] START var_smoothing=1e-12.......................................\n",
      "[CV 2/2; 11/14] END var_smoothing=1e-12;, score=(train=0.307, test=0.253) total time=   0.1s\n",
      "[CV 1/2; 12/14] START var_smoothing=1e-13.......................................\n",
      "[CV 1/2; 12/14] END var_smoothing=1e-13;, score=(train=0.227, test=0.187) total time=   0.1s\n",
      "[CV 2/2; 12/14] START var_smoothing=1e-13.......................................\n",
      "[CV 2/2; 12/14] END var_smoothing=1e-13;, score=(train=0.291, test=0.240) total time=   0.1s\n",
      "[CV 1/2; 13/14] START var_smoothing=1e-14.......................................\n",
      "[CV 1/2; 13/14] END var_smoothing=1e-14;, score=(train=0.223, test=0.184) total time=   0.1s\n",
      "[CV 2/2; 13/14] START var_smoothing=1e-14.......................................\n",
      "[CV 2/2; 13/14] END var_smoothing=1e-14;, score=(train=0.282, test=0.230) total time=   0.1s\n",
      "[CV 1/2; 14/14] START var_smoothing=1e-15.......................................\n",
      "[CV 1/2; 14/14] END var_smoothing=1e-15;, score=(train=0.217, test=0.178) total time=   0.1s\n",
      "[CV 2/2; 14/14] START var_smoothing=1e-15.......................................\n",
      "[CV 2/2; 14/14] END var_smoothing=1e-15;, score=(train=0.277, test=0.223) total time=   0.1s\n",
      "best parameters:  {'var_smoothing': 0.01}\n",
      "score:  0.3989583333333333\n",
      "['Rock' 'Country' 'Spoken' 'Rock' 'Hip-Hop' 'Rock' 'Electronic' 'Hip-Hop'\n",
      " 'Rock' 'Pop']\n",
      "track_id\n",
      "17725            Rock\n",
      "9554             Rock\n",
      "4514     Experimental\n",
      "4801             Rock\n",
      "11800         Hip-Hop\n",
      "18687            Rock\n",
      "9091       Electronic\n",
      "12369         Hip-Hop\n",
      "10149            Rock\n",
      "486              Rock\n",
      "Name: genre_top, dtype: category\n",
      "Categories (16, object): ['Blues', 'Classical', 'Country', 'Easy Listening', ..., 'Pop', 'Rock', 'Soul-RnB', 'Spoken']\n",
      "                0              1             2              3              4   \\\n",
      "0     0.000000e+00   0.000000e+00  0.000000e+00   3.548764e-33   2.485091e-09   \n",
      "1     3.976117e-18  1.227655e-142  1.000000e+00  1.734333e-114   2.722360e-88   \n",
      "2     2.693623e-62  6.825563e-122  4.289186e-48  9.585169e-135  1.452708e-113   \n",
      "3     1.030596e-99   0.000000e+00  1.832222e-59   3.183362e-95   3.573536e-63   \n",
      "4     6.811080e-73  1.910787e-238  1.346056e-56  9.521723e-122  2.483160e-100   \n",
      "...            ...            ...           ...            ...            ...   \n",
      "3195  1.120235e-78  1.257623e-259  2.142324e-62   2.073944e-91   1.126897e-73   \n",
      "3196  3.473367e-66  1.173789e-218  2.696591e-14  4.748747e-105   1.826946e-72   \n",
      "3197  0.000000e+00  3.527884e-232  3.830593e-91   6.075683e-99   4.096371e-56   \n",
      "3198  9.326231e-21  8.590763e-161  1.000000e+00  1.211413e-131   1.220579e-95   \n",
      "3199  6.458450e-18  3.629181e-109  6.150849e-06  2.286696e-121   1.632886e-86   \n",
      "\n",
      "                 5              6              7              8   \\\n",
      "0     2.048092e-115  4.029228e-198   1.050334e-06  3.188389e-278   \n",
      "1      3.502857e-58   7.165861e-25   1.976074e-98   4.643373e-28   \n",
      "2     4.979932e-110   9.520398e-34  3.600446e-137   1.771575e-39   \n",
      "3      5.574192e-81   1.417863e-26   5.665632e-78   5.371564e-99   \n",
      "4     1.947297e-104   1.000000e+00  7.132180e-126   1.351072e-71   \n",
      "...             ...            ...            ...            ...   \n",
      "3195  2.910982e-102   2.436031e-20   9.556735e-84   3.568407e-56   \n",
      "3196   5.299963e-68   6.752557e-17   4.118964e-86   2.534449e-47   \n",
      "3197   1.323137e-44   2.011516e-42   5.548932e-66   1.020631e-24   \n",
      "3198   1.655496e-74   2.396040e-56  1.223195e-110   2.330593e-38   \n",
      "3199   1.538919e-72   9.428664e-18  1.595414e-103   4.119179e-13   \n",
      "\n",
      "                 9              10            11            12            13  \\\n",
      "0     4.563867e-237   0.000000e+00  1.109356e-44  9.999989e-01  0.000000e+00   \n",
      "1      1.368999e-27  1.096657e-212  3.088425e-21  1.023190e-33  8.157357e-31   \n",
      "2      6.358342e-29  2.765899e-275  3.681987e-67  2.596067e-94  2.415327e-33   \n",
      "3     2.726677e-104  2.809296e-286  3.430967e-25  1.000000e+00  1.348546e-99   \n",
      "4      3.838221e-70  4.965700e-260  8.427429e-42  8.959397e-46  4.417884e-50   \n",
      "...             ...            ...           ...           ...           ...   \n",
      "3195   7.127674e-53  8.502013e-272  7.608570e-26  4.227763e-18  1.000000e+00   \n",
      "3196   1.221256e-57  1.205239e-184  4.559645e-15  1.000000e+00  2.007606e-24   \n",
      "3197   6.665102e-47   9.706350e-91  1.017347e-11  1.000000e+00  0.000000e+00   \n",
      "3198   1.626075e-42  4.467188e-139  2.063118e-36  5.758752e-24  6.349613e-33   \n",
      "3199   8.543925e-18   6.817213e-93  7.459718e-35  5.571182e-39  1.653397e-16   \n",
      "\n",
      "                 14  \n",
      "0     2.426534e-245  \n",
      "1      8.330190e-49  \n",
      "2      1.000000e+00  \n",
      "3      2.546945e-89  \n",
      "4      4.420219e-57  \n",
      "...             ...  \n",
      "3195   1.707146e-59  \n",
      "3196   5.337368e-38  \n",
      "3197   1.160781e-42  \n",
      "3198   6.963318e-53  \n",
      "3199   9.999938e-01  \n",
      "\n",
      "[3200 rows x 15 columns]\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "              Blues       0.04      0.29      0.07        21\n",
      "          Classical       0.50      0.81      0.62        63\n",
      "            Country       0.08      0.58      0.14        36\n",
      "         Electronic       0.69      0.15      0.25       464\n",
      "       Experimental       0.45      0.18      0.25       491\n",
      "               Folk       0.40      0.24      0.30       270\n",
      "            Hip-Hop       0.35      0.69      0.47       192\n",
      "       Instrumental       0.13      0.20      0.16        45\n",
      "      International       0.13      0.34      0.19        77\n",
      "               Jazz       0.12      0.32      0.17        74\n",
      "Old-Time / Historic       0.76      0.90      0.82        81\n",
      "                Pop       0.04      0.05      0.04        79\n",
      "               Rock       0.73      0.45      0.56      1236\n",
      "           Soul-RnB       0.07      0.61      0.13        18\n",
      "             Spoken       0.16      0.79      0.26        53\n",
      "\n",
      "           accuracy                           0.37      3200\n",
      "          macro avg       0.31      0.44      0.30      3200\n",
      "       weighted avg       0.55      0.37      0.40      3200\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "GaussianNB(var_smoothing=0.01)",
      "text/html": "<style>#sk-container-id-8 {color: black;background-color: white;}#sk-container-id-8 pre{padding: 0;}#sk-container-id-8 div.sk-toggleable {background-color: white;}#sk-container-id-8 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-8 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-8 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-8 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-8 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-8 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-8 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-8 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-8 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-8 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-8 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-8 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-8 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-8 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-8 div.sk-item {position: relative;z-index: 1;}#sk-container-id-8 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-8 div.sk-item::before, #sk-container-id-8 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-8 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-8 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-8 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-8 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-8 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-8 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-8 div.sk-label-container {text-align: center;}#sk-container-id-8 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-8 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-8\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GaussianNB(var_smoothing=0.01)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" checked><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GaussianNB</label><div class=\"sk-toggleable__content\"><pre>GaussianNB(var_smoothing=0.01)</pre></div></div></div></div></div>"
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# naive bayes model evaluation\n",
    "param_evaluated = {\n",
    "    'var_smoothing': [1e-2, 1e-3, 1e-4, 1e-5, 1e-6, 1e-7, 1e-8, 1e-9, 1e-10, 1e-11, 1e-12, 1e-13, 1e-14, 1e-15]\n",
    "}\n",
    "\n",
    "nb_base_model = GaussianNB()\n",
    "nb_best_model = optimize_model(nb_base_model, X_train, y_train, param_evaluated)\n",
    "evaluate_classifier(X_train, X_test, y_train, y_test, nb_best_model)\n",
    "nb_best_model\n",
    "\n",
    "# After run\n",
    "# best parameters:  {'var_smoothing': 0.01}\n",
    "#score:  0.3989583333333333"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h1>Neural network</h1>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Split the data in training and remaining dataset\n",
    "X_train, X_rem, y_train, y_rem = train_test_split(features_df, track_genres, train_size=0.8, random_state=42, stratify=track_genres)\n",
    "\n",
    "# Split remaining dataset in test and validation\n",
    "test_size = 0.5\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(X_rem, y_rem, test_size=0.5)\n",
    "\n",
    "print(f\"Training has {len(X_train), len(y_train)}, Validation has {len(X_valid), len(y_valid)}, Testing has {len(X_test), len(y_test)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train = scaler.transform(X_train)\n",
    "X_valid = scaler.transform(X_valid)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "lab_encoder = LabelEncoder()\n",
    "lab_encoder.fit(y_train)\n",
    "\n",
    "y_train = lab_encoder.transform(y_train)\n",
    "y_valid = lab_encoder.transform(y_valid)\n",
    "y_test = lab_encoder.transform(y_test)\n",
    "\n",
    "y_train = tf.keras.utils.to_categorical(y_train)\n",
    "y_valid = tf.keras.utils.to_categorical(y_valid)\n",
    "y_test = tf.keras.utils.to_categorical(y_test)\n",
    "\n",
    "X_train.shape, y_train.shape, X_valid.shape, y_valid.shape, X_test.shape, y_test.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(256, activation='relu', input_shape=(518,)))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "print(len(y_train[1]))\n",
    "model.add(Dense(len(y_train[1]), activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "keras.utils.plot_model(model, show_shapes=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "early_stop = EarlyStopping(monitor='val_loss',\n",
    "                           patience=10,\n",
    "                           restore_best_weights=True,\n",
    "                           mode='min')\n",
    "\n",
    "history = model.fit(X_train, y_train, validation_data=(X_valid, y_valid), epochs=50, callbacks=[early_stop, PlotLossesKeras()])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.evaluate(X_test, y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def model_builder(hp):\n",
    "    model = keras.Sequential()\n",
    "\n",
    "    # Tune the number of units in the first Dense layer\n",
    "    # Choose an optimal value between 32-512\n",
    "    hp_units1 = hp.Int('units1', min_value=32, max_value=512, step=32)\n",
    "    model.add(Dense(hp_units1, activation='relu', input_shape=(518,)))\n",
    "    hp_units2 = hp.Int('units2', min_value=32, max_value=512, step=32)\n",
    "    model.add(Dense(hp_units2, activation='relu'))\n",
    "\n",
    "    model.add(Dense(len(y_train[1]), activation='softmax'))\n",
    "    # Tune the learning rate for the optimizer\n",
    "    # Choose an optimal value from 0.01, 0.001, or 0.0001\n",
    "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
    "\n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate),\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "tuner = kt.Hyperband(model_builder,\n",
    "                     objective='val_accuracy',\n",
    "                     max_epochs=10,\n",
    "                     factor=3,\n",
    "                     directory='my_dir',\n",
    "                     project_name='intro_to_kt')\n",
    "\n",
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "tuner.search(X_train, y_train, epochs=50, validation_split=0.2, callbacks=[stop_early])\n",
    "\n",
    "# Get the optimal hyperparameters\n",
    "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(f\"\"\"\n",
    "The hyperparameter search is complete. The optimal number of units in the first densely-connected\n",
    "layer is {best_hps.get('units')} and the optimal learning rate for the optimizer\n",
    "is {best_hps.get('learning_rate')}.\n",
    "\"\"\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Build the model with the optimal hyperparameters and train it on the data for 50 epochs\n",
    "model = tuner.hypermodel.build(best_hps)\n",
    "history = model.fit(X_train, y_train, epochs=10, validation_data=(X_valid, y_valid))\n",
    "\n",
    "val_acc_per_epoch = history.history['val_accuracy']\n",
    "best_epoch = val_acc_per_epoch.index(max(val_acc_per_epoch)) + 1\n",
    "print('Best epoch: %d' % (best_epoch,))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "hypermodel = tuner.hypermodel.build(best_hps)\n",
    "\n",
    "# Retrain the model\n",
    "hypermodel.fit(X_train, y_train, epochs=best_epoch, validation_data=(X_valid, y_valid), callbacks=[PlotLossesKeras()])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "hypermodel.evaluate(X_test, y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}